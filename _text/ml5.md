---
section: ml
title: "Предварительный анализ и обработка данных"
---

Еще в самом начале книги мы говорили о том, что в машинном обучении самый главный и критичный компонент - сами данные, на которых проводится обучение. Качество данных напрямую влияет на эффективность и надежность получаемых моделей. Поэтому значительная часть работы аналитика - это сбор, очистка, анализ и улучшение данных. В реальных проектах на это уходит до 80% всего времени, затраченного на проект. И лишь оставшиеся 20% - это обучение и выбор моделей, диагностика и повышение их эффективности.

Конечно, работа с данными предваряет непосредственно моделирование. Поэтому во многих курсах с этого как раз начинают. Но как показывает опыт преподавания машинного обучения, очень сложно понять некоторые аспекты, важные для процесса предварительной обработки данных, без понимания того, для чего эти данные будут использоваться, каковы особенности разных моделей, то есть тех тем, которые мы рассматривали в предыдущих главах. Именно поэтому в этой главе ма познакомимся с процессами и методами, которые предшествуют построению моделей машинного обучения. Здесь мы поговорим о сборе, анализе и обработке данных для моделирования. 

Надо сказать, что мы не сможем описать все многообразие методов анализа данных. Здесь мы поговорим именно про подготовку данных для машинного обучения и про типичные задачи, которые надо решить для этого, типичные вопросы, на которые нужно ответить. Но анализ данных в широком смысле гораздо обширнее и глубже этого, это целая отдельная дисциплина, основанная на математической статистике, численных методах и алгоритмах. Конечно, в первую очередь данные нужно собрать, а для этого нужно понять, какие данные нужны для решения конкретной задачи, определить требования к ним. Так что начнем мы с самого первого шага - сбора данных.

### Сбор данных для обучения

#### Понятие чистых данных

В первую очередь необходимо зафиксировать, в какой форме собираются и хранятся данные для машинного обучения. Многое их этого мы уже видели и на примерах, и немного определяли математически, но сейчас пора явно сформулировать требования к представлению данных и их структуре. Обработка и анализ неструктурированной информации - это отдельная тема, которой мы не будем касаться. Для классического машинного обучения данные должны иметь вполне определенную структуру - чаще всего табличную.

Датасет - набор данных для машинного обучения - состоит из описания некоторого конечного множества объектов, экспериментов и измерений. Так как машинное обучение не привязано к какой-то определенной предметной области, данные могут описывать все, что угодно. Объекты недвижимости, определенных людей, финансовые транзакции, фотографии каких-то объектов, данные с телеметрии спутников. Но главное - что эта информация описывает набор каких-то отдельных объектов реального мира, элементарных сущностей той предметной области, которую мы моделируем. Еще эти объекты называют точками данных, или строками, так как в таблицах они всегда отображаются именно по строкам.

У каждого из этих объектов есть некоторые характеристики, которые описаны в датасете. Для всех объектов набор этих характеристик должен быть одинаковый. Эти характеристики реальных объектов называются атрибутами или колонками, опять же потому, что в табличном представлении они располагаются по столбцам. Иногда они же называются переменными, либо факторами. Соответственно, на пересечении столбца и строки находятся значения соответствующих характеристик определенного объекта.

![](/assets/images/ml_text/ml5-1.png "dataset"){: .align-center style="width: 800px;"}

По сути такое представление информации очень похоже на реляционную форму, которая используется в базах данных. Главное различие в том, что в машинном обучении датасет представляется в виде единой таблицы, а не набора связанных, как в реляционных базах данных. Такая таблица должна иметь внутреннюю согласованность, то есть описывать один вид наблюдений или экспериментов. Причем внутренняя согласованность должна проявляться и в содержании данных. В датасете должны использоваться одни обозначения, сокращения, форматы. Мало толку от таблицы, в которой часть объектов измерена в метрах, а остальные в футах. Если мы анализируем экономические показатели, то агрегаты должны измеряться за одни и те же периоды и так далее. Таких маленьких нюансов в данных может быть очень много и для их отслеживания необходимо знание предметной области.

Если датасет удовлетворяет этим базовым требованиям, то такие данные можно называть чистыми (tidy data). На практике к базовым добавляются еще два требований, которые почти всегда нужно соблюдать, иначе большинство моделей машинного обучения не смогут работать с такими данными. Во-первых, данные должны быть выражены в численном виде. Про то, как преобразовать данные в численный вид мы поговорим чуть позже, в разделе про предобработку данных. Это требование объясняется тем, что большинство моделей основаны на численных функциях и математических преобразованиях. Поэтому на вход надо подавать набор численных значений, или вектор. 

Во-вторых, в данных не должно быть отсутствующих значений, то есть пропусков. Все "ячейки" таблицы должны быть заполнены. Это тоже обосновано применением математических моделей, которые не умеют работать с отсутствием значения. Заполнению пропущенных значений тоже посвящена большая часть предобработки данных. Вообще, приведение данных к такому чистому виду - это основная цель предварительной обработки данных как этапа проекта по машинному обучению.

Конечно, на практике бывают и исключения из этих правил. Существуют специальные области, в которых данные представляются в особой структуре и в особых форматах. Типичный пример - временные ряды. Это ряд чисел, которые характеризуют динамику одного и того же показателя во времени. То есть датасет в таком случае представляется не как набор взаимонезависимых объектов. Элементы в таком временном ряду имеют строгую последовательность, которую нельзя нарушать. В предыдущей главе мы уже говорили, что временные ряды нельзя разбивать на обучающую и тестовую выборки случайным образом. Точно также следует учитывать особенности природы исследуемых данных и в других случаях.

Еще один распространенный пример - нестандартные форматы данных. В машинном обучении часто встречается ситуация, когда отдельная точка данных представляет собой не численный вектор, а, например, картинку, текст, видеофрагмент или аудиоданные. Но и в таком случае, для моделирования эти данные преобразуются к численным векторам. Этот процесс называется векторизацией. Так можно векторизовать текст, для этого есть много разных методов. Картинки, очень естественно представляются как массив яркости пикселей, подобным же образом преобразуются и другие типы данных. Просто преобразование данных в этом случае не так тривиально и может быть предметом отдельного исследования.

{% capture notice %}
Надо отметить, что в подобных задачах алгоритм векторизации данных может быть даже более важным для моделирования, чем сами модели машинного обучения. Так, в обработке текстов на естественных языках, существуют разные методы преобразования текста в вектор. Самые простые подсчитывают количество и наличие слов в тексте, более продвинутые - учитывают грамматическую форму и смысл слов и предложений текста. Так вот, для совершенно разных задач обработки текстов - будь то машинный перевод или вопрос-ответные системы, гораздо большее влияние на эффективность модели оказываем именно метод векторизации. Поэтому создание так называемых глубоких текстовых моделей, которые позволяют преобразовать текст в вектор так, чтобы в этом векторе сохранилась максимум информация о смысле текста, привело к одновременному прогрессу во многих прикладных задачах.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Вообще, для анализа такой специфической информации существуют и специальные методы и модели. Например, для анализа графической информации традиционно применяются так называемые алгоритмы сверток, которые учитывают пространственное расположение информации на картинке. А анализ временных рядов - это вообще отдельная дисциплина в математической статистике, эконометрике и анализе данных. Но мы в этой книге сконцентрируемся на анализе более традиционной, табличной формы данных. Тем более, что она наиболее распространена в прикладных экономических, производственных и технических задачах.

{% capture notice %}
Выводы:
1. Датасет - это набор данных, используемый для обучения моделей. Данные представлены в виде единой таблицы.
1. Объекты - это элементарные сущности, которые мы изучаем, объекты реального мира, измерения, наблюдения. В датасете представляются строками.
1. Каждый объект характеризуется набором атрибутов. В датасете атрибуты представляются столбцами.
1. Каждая таблица, файл представляет собой данные об одном виде наблюдений или экспериментов и должна иметь внутреннюю согласованность.
1. Дополнительно: Все данные должны быть выражены в численном виде.
1. Дополнительно: В данных не должно быть отсутствующих (пропущенных) значений. 
1. Существуют специальные структуры данных и форматы.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Оценка источников и объемов данных

Теперь, когда мы понимаем, в какой форме нам нужных данные пора задуматься о том, сколько их требуется и откуда их брать. Оценка необходимых объемов данных - очень сложная задача. Тем более, что этот вопрос очень напрашивается перед началом работы - сколько именно данных нужно для эффективного моделирования. Но ответить на него очень сложно, ведь алгоритмы машинного обучения не предъявляют требований к размеру выборки. Самый универсальный ответ, который можно использовать - чем больше, тем лучше. Мы уже знакомы с недо- и переобучением моделей, и поэтому знаем, что добавление большего количества точек данных может помочь в первом случае, но при этом во втором тоже не пойдет во вред. Поэтому если заранее не знать, то добавление данных - в любом случае не повредит. Но обычно нам доступны лишь ограниченные данные, а если нам нужно больше - придется прилагать определенные усилия по их поиску, добычи или генерации. Поэтому вопрос необходимого объема - не праздный. Если собрать маленькую выборку может быть относительно просто и быстро, то увеличить ее в сто раз может стоить очень дорого, как по ресурсам, так и по времени. 

{% capture notice %}
Под объемом датасета мы будем понимать количество точек в данных, а не объем информации в байтах. Количество точек или объектов - это ключевая характеристика наборов данных для машинного обучения.
{% endcapture %}
<div class="notice--success">{{ notice | markdownify }}</div>

В оценке требуемого объема данных следует в первую очередь ориентироваться на порядок ложности задачи, которую мы пытаемся решить. Чем задача сложнее, тем больше данных нам потребуется. Это интуитивно понятно - для описания принципов решения сложной задачи может понадобится больше примеров, чем для простой, чтобы модель смогла обобщить эти примеры и выдавать приемлемый результат. Проблема в том, что порядок сложности задачи тоже нелегко определить заранее. Тут поможет только сравнительный анализ. Прикладные задачи машинного обучения можно сравнивать между собой по сути моделируемого вопроса. Например, известно, что задача машинного перевода - достаточно сложная и требует очень сложной и вариативной модели и, как следствие, огромного количества данных. Поэтому если вы строите модель, которая решает сходные задачи, следует ожидать, что парой тысяч примеров не отделаться. С другой стороны, задача распознавания простых визуальных образов - существенно более простая, и здесь может не понадобится гигантских объемов выборки, можно для начала ограничится парой тысяч точек.

Тут загвоздка еще в том, что сложность задачи сама по себе зависит от состава имеющихся данных. Если какой-то атрибут позволяет с высокой долей уверенности предсказывать значение целевой переменной, это сильно понижает уровень сложности задачи. И, как следствие, может хватить гораздо меньшего по объему датасета. Если же между признаками и целевой переменная связь очень слабая, то сложность наоборот, растет, как растет и требование к количеству данных. Поэтому большие усилия исследователей направлены на повышение информативности и полезности признаков, извлекаемых из данных.

Соображения, учитывающие сложность задачи, кажутся запутанными и неконкретными. Поэтому есть еще один ориентир, который может помочь в оценке необходимого объема данных. Выборка должна быть репрезентативной, то есть представлять все возможные случаи из генеральной совокупности. Если известно, хотя бы примерно, какую именно информацию об объектах предметной области можно собирать (то есть, какие атрибуты будут в датасете), то можно порассуждать, какие комбинации значений этих атрибутов встречаются в реальной жизни. Они все должны быть представлены в наборе данных, причем, желательно, по нескольку раз, так как набор данных будет подвергаться дроблению и разделению. 

Например, рассмотрим задачу определения эмоций по звуку голоса человека. Как может рассуждать исследователь, перед которым стоит задача сбора данных? Во-первых, нам нужно собрать в датасет образцы голоса для разных эмоций. При этом не существует какого-то единого классификатора или набора эмоций, здесь надо исходить из контекста задачи и доступных источников данных. Во-вторых, нужны образцы разных голосов - женские, мужские, высокие, низкие. Причем, на каждый голос нужно несколько примеров со всеми эмоциями. Нужны ли записи на разных языках? Можно выдвинуть гипотезу, что проявления эмоций в голосе не зависят от языка, поэтому можно ограничиться одним или двумя языками. Итого у нас получается, что надо собрать максимальное количество голосов разных людей и для каждого голоса включить в выборку несколько примеров на разные эмоции. Допустим, мы решили выбрать голоса 20 человек. В нашем исследовании мы распознаем 8 эмоций. И для надежности нужно по 10 звуковых фрагментов на каждую комбинацию. Получается 1600 фрагментов. Если добавить второй язык, то желаемый объем датасета приблизится к трем тысячам точек. 

И это не значит, что при таком объеме модель точно будет работать эффективно, это лишь необходимый минимум, с которого можно начать. При таком подходе мы обеспечиваем минимальную репрезентативность выборки, а уже после первоначального обучения, проведя диагностику модели, можно сделать вывод, хватает нам данных или нужно собрать кратно больше. Это такой быстрый черновой расчет, который может подсказать хотя бы примерный порядок количества точек, при которых вообще может идти речь о целесообразности моделирования. Иначе вся дальнейшая работа будет просто бессмысленной: если мы оценили требуемый объем в миллион точек, а нам доступны только пять тысяч, то даже не стоит и начинать более глубокий анализ.

На практике стоит учитывать не только пожелания к объему данных, но и имеющиеся в распоряжении аналитика источники. Подбор и поиск источников данных тоже очень важен для любого практического проекта. В самой первой главе мы говорили, что поиск данных является основным сдерживающим фактором, который не позволяет использовать машинное обучение в некоторых сферах. Поэтому перед началом анализа и моделирования следует определить, откуда можно будет брать те самые данные. Причем эту работу стоит проводить параллельно с оценкой объемов, так как именно доступные источники подскажут, какую именно информацию удастся собрать, а это может повлиять на сложность задачи. 

{% capture notice %}
При поиске данных нужно учитывать, что возможно комбинировать данные из разных датасетов. При этом можно использовать данные по немного другой задаче, смежной с решаемой, а затем дообучить модель на нужной задаче. Такой подход называется трансфер обучения, это одна из продвинутых техник, но имейте в виду, что такое в принципе возможно.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

В первую очередь стоит помнить, что источники данных могут быть внешние и внутренние. Внутренние источники - это различные корпоративные информационные система, базы данных, базы знаний, хранилища данных. В первую очередь следует подумать, какие данные по стоящей перед нами задачи уже имеются у организации и могут быть использованы. Зачастую этого бывает вполне достаточно. Если же нет, то тогда приходится обращаться к внешним источникам. Существует огромное количество готовых датасетов для решения типичных задач машинного обучения - от распознавания картинок до моделирования временных рядов цен активов. Можно попробовать найти датасет по нужной или сходной тематике на таких сайтах ка Kaggle или PapersWithCode, где публикуются целые библиотеки датасетов по рубрикам. Также часто датасеты публикуются на сайтах образовательных, научных организаций и подразделений, которые ведут исследовательскую работу. В конце концов можно искать ссылки на данные в научных публикациях, которые можно найти в специализированных системах научного цитирования типа Google Scholar.

{% capture notice %}
Особенной проблемой корпоративных хранилищ данных, тем более в крупных организациях, является непоследовательность наименований, сокращений, условных обозначений. Это большая проблема при интеграции данных и она отнимает огромное количество сил именно на этапе предобработки, а также может приводить к большому количеству ошибок в данных.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

При этом внешние источники информации могут быть в ограниченном доступе - например, платные или по подписке. Достоинство внешних источников данных в том, что обычно там публикуются уже готовые датасеты, которые не требуют интенсивной очистки и анализа, возможно только минимальной доводки и проверки на чистоту. В то время как информация из внутренних баз данных обычно не предназначается для машинного обучения, поэтому ее обработка может занять большое количество времени.

{% capture notice %}
При сборе данных следует разделять потоковые и статические данные. Статические, или пакетные данные - это информация, которая дана в полном объеме сразу. С ней работать гораздо легче. Потоковые данные - генерируются в реальном времени с определенной скоростью. Если вам необходимо работать именно с потоковыми данными, то для этого существуют специальные техники, такие как онлайн обучение, пассивно-агрессивные модели и конвейеризация обработки данных.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Данные вообще можно не только собирать. Довольно часто по какой-то конкретной задаче вообще нет необходимых данных, либо они присутствуют в совершенно недостаточном количестве. Тогда приходится задумываться о самостоятельной генерации таких данных. Например, если вы занимаетесь распознаванием определенной категории товаров по картинке для магазина, вполне можно сделать большое количество фотографий самостоятельно. Генерация данных  - процесс, конечно, более трудоемкий, чем сбор уже существующих данных, но зато в итоге может получиться датасет, который специально подходит для поставленной конкретной задачи, и он позволит строить более точные модели именно для этой задачи.

Довольно часто бывает, что собранных данных категорически недостаточно для проведения моделирования. В таком случае может помочь один прием, которых называется аугментацией данных (data augmentation). Он позволяет из одной точки данных потенциально получить несколько и таким образом кратно увеличить объем имеющегося датасета. Например, мы рассматриваем задачу распознавания рукописных символов и собрали небольшой датасет из таких картинок, но пришли к выводу, что нам нужно в разы больше данных. Каждую картинку в датасете можно повернуть на небольшой угол, изменить размер, сместить. То есть придумать какие-то небольшие изменения, модификации, которые позволяют из одной имеющейся картинки получить, потенциально, несколько десятков вариантов. Если мы применим эти модификации, то увеличим количество точек данных в датасете в такое количество раз, сколько модификаций можно применить к каждой точке. Этот прием имеет еще один положительный эффект на процесс моделирования: если модель обучается на датасете, в котором присутствуют разные варианты одной и той же картинки с разными сдвигами, то модель получается более устойчива к таким сдвигам. То есть мы не просто увеличиваем объем датасета, но еще и получаем более надежные и качественные модели. Понятно, что стратегия аугментации данных очень сильно зависит от конкретной решаемой задачи: для распознавания картинок и определения цен на сложные товары сами модификации будут принципиально разные. Но на практике аугментация данных - это очень полезный прием.

Еще одна задача, с которой часто сталкиваются при сборе данных для моделирования - разметка данных. Для обучения с учителем, как мы знаем, в датасете должны присутствовать "правильные ответы" для каждой точки выборки. Но мы можем иметь доступ к таким данным, которые очень подходят для решения задачи, но в них отсутствует именно значения целевой переменной, или такие значения имеются лишь частично. Например, решаем задачу определения мошеннических транзакций. В организации есть достаточно большая история транзакций, которую можно было бы использовать для обучения. Но в ней нет информации, какие именно транзакции считаются "подозрительными". Такие данные называются неразмеченными (unlabelled). Было бы жалко не использовать такие прекрасные данных. В таком случае можно задуматься о ручной разметке данных - то есть специалист, эксперт предметной области,  должен указать, какие транзакции считаются подозрительными, а какие - обычными. Ручная разметка данных это очень трудозатратный процесс, который зачастую требует работы нескольких экспертов. Также следует помнить, что любые экспертные оценки могут содержать ошибки. Но в некоторых случаях ручная разметка данных - единственный способ получить необходимую обучающую выборку для решения определенной задачи. 

В итоге можно назвать основные способы получения данных для машинного обучения. В качестве рекомендации приведем ориентировочный порядок, в котором имеет смысл рассматривать эти способы в решении прикладной задачи:

{% capture block %}
1. Сбор данных из внешних открытых источников
1. Сбор данных из внутренних источников
1. Аугментация данных
1. Разметка данных 
1. Сбор данных из внешних закрытых и платных источников
1. Генерация данных
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

В этом списке способы выстроены от менее затратных к более. Поэтому целесообразно рассматривать сначала первый вариант из списка и переходить к следующим только если он не дает нужного результата - достаточного количества данных. Конечно, этот список примерный, в реальных задачах порядок этих способов может меняться.

{% capture notice %}
Выводы:
1. После постановки задачи машинного обучения первый этап моделирования - определение источников и объема данных, необходимых для эффективного обучения.
1. Объем данных определяется порядком сложности задачи: чем больше данных, тем более сложные модели можно на них строить.
1. В целом, чем больше данных можно собрать, тем лучше.
1. Данные в датасете должны быть репрезентативной выборкой генеральной совокупности.
1. Источники данных могут быть открытые и закрытые, платные, по подписке, внутренние и внешние.
1. Данные могут быть пакетные и потоковые, с ними надо работать по разному.
1. Данные можно не только собирать, но еще генерировать и модифицировать.
1. Существующие данные может потребоваться размечать.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Интеграция данных

Как мы говорили, для машинного обучения чем больше данных мы соберем, тем потенциально лучшие модели мы сможем получать. Поэтому следует собрать всю информацию о предметной области, которую возможно. При этом, напоминаем, что датасет для обучения модели должен быть представлен в виде одной таблицы. А после сбора информации мы можем иметь множество таблиц, особенно, если собирали информацию из разных источников. Но даже в одном источнике нужная информация может быть разнесена по нескольким таблицам, как в реляционной базе данных. Поэтому после сбора данных у нас может быть множество разных таблиц, файлов и хранилищ данных, которые потенциально могут быть полезны. Поэтому следующий этап - это интеграция данных, то есть сведение всех разрозненных кусочков информации в единый датасет.

Объединение множества таблиц можно рассматривать поэтапно. На каждом этапе две таблицы объединяются в одну. Следуем в первую очередь объединять таблицы, наиболее близкие по смыслу и происхождению. При этом каждое такое объединение может быть двух видов: объединение по вертикали и по горизонтали. Рассмотрим эти две элементарные операции по отдельности.

Вертикальное объединение данных нужно тогда, когда в двух талицах имеется примерно одна и та же информация, но про разные объекты. То есть в двух таблицах разные строчки данных, но примерно одинаковые столбцы. И в таком случае нужно "склеить" эти две таблицы по вертикали, вот так:

{% capture block %}
<table>
    <tr>
        <td>
            <table>
                <tr>
                    <td>id</td>
                    <td>height</td>
                    <td>size</td>
                    <td>label</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>182</td>
                    <td>52</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>158</td>
                    <td>50</td>
                    <td>1</td>
                </tr>
            </table>
        </td>
        <td>
            +
        </td>
        <td>
            <table>
                <tr>
                    <td>id</td>
                    <td>height</td>
                    <td>size</td>
                    <td>wight</td>
                    <td>label</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>180</td>
                    <td>48</td>
                    <td>74</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>153</td>
                    <td>46</td>
                    <td>73</td>
                    <td>1</td>
                </tr>
            </table>
        </td>
        <td>
            =
        </td>
        <td>
            <table>
                <tr>
                    <td>id</td>
                    <td>height</td>
                    <td>size</td>
                    <td>wight</td>
                    <td>label</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>182</td>
                    <td>52</td>
                    <td>NaN</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>158</td>
                    <td>50</td>
                    <td>NaN</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>180</td>
                    <td>48</td>
                    <td>74</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>153</td>
                    <td>46</td>
                    <td>73</td>
                    <td>1</td>
                </tr>
            </table>
        </td>
    </tr>
</table>
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

В этом примере мы склеиваем две таблицы по вертикали. При этом получается талица, которая состоит из всех строк первой плюс из всех строк второй. А что происходит со столбцами? Для общих столбцов, то есть таких, которые есть в обеих таблицах все просто - они переносятся в таблицу-результат. А что делать со столбцами, которые присутствуют только в одной из двух таблиц-"слагаемых", надо решать аналитику. По умолчанию, в получившейся таблице будут присутствовать все столбцы из обеих исходных таблиц. Но в ячейках, которые относились к той таблице, в которой такого столбца нет, будут записаны пропуски. В нашем примере такой столбец один - "weight" из второй таблицы. В результате в тех ячейках этого столбца, который относились к первой таблице записано специальное значение "NaN" (Not a number) - специальное обозначение пропущенного значения. 

{% capture notice %}
В библиотеках работы с табличной и статистической информацией для простого склеивания таблиц, нужного для вертикального объединения данных существуют специальные методы, такие как _append()_ или _concat()_ в _pandas_.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Конечно, можно представить ситуацию, когда такие "непарные" столбцы имеются в обеих исходных таблицах. По сути такая ситуация происходит, когда в разных таблицах отражена немного разная информация об объектах предметной области. Такое постоянно случается при интеграции данных из разных источников. По желанию аналитика можно настроить метод объединения таблиц таким образом, чтобы такие "спорные" столбцы вообще не включались в итоговую таблицу. То есть в результате она будет содержать только те атрибуты, которые присутствовали в обеих исходных таблицах, но зато в ней гарантировано не будет пропусков (если только их не было в исходных таблицах).

Горизонтальное объединение данных нужно, когда в исходных таблицах содержится разная информация о примерно тех же объектах предметной области. То есть в исходных таблицах примерно те же строки, но совершенно разные столбцы. Такая ситуация тоже часто случается, но обычно при объединении данных из одного источника. Например, из корпоративных реляционных хранилищ данных.

Горизонтальное объединение данных более сложно по своей природе, чем вертикальное. Дело в том, что так таблицы нельзя просто склеить - надо следить за соответствием строк. Ведь в разных таблицах могут присутствовать немного разные строки, а те, что есть в обоих - храниться в разном порядке. В реляционных базах данных для таких случаев есть специальная операция - соединение таблиц (join). Рассмотрим в качестве примера две таблицы с информацией о ценах и объемах торгов двух разных акций на бирже:

{% capture block %}
<table>
    <tr>
        <td>
            <table>
                <tr>
                    <td><b>date</b></td>
                    <td>price</td>
                    <td>volume</td>
                </tr>
                <tr>
                    <td><b>20/01</b></td>
                    <td>14.56</td>
                    <td>135</td>
                </tr>
                <tr>
                    <td><b>21/01</b></td>
                    <td>15.23</td>
                    <td>280</td>
                </tr>
                <tr>
                    <td><b>22/01</b></td>
                    <td>15.07</td>
                    <td>73</td>
                </tr>
                <tr>
                    <td><b>23/01</b></td>
                    <td>15.56</td>
                    <td>160</td>
                </tr>
                <tr>
                    <td><b>24/01</b></td>
                    <td>17.85</td>
                    <td>1450</td>
                </tr>
            </table>
        </td>
        <td>
            +
        </td>
        <td>
            <table>
                <tr>
                    <td><b>date</b></td>
                    <td>price</td>
                    <td>volume</td>
                </tr>
                <tr>
                    <td><b>19/01</b></td>
                    <td>23 064</td>
                    <td>15 067 894</td>
                </tr>
                <tr>
                    <td><b>20/01</b></td>
                    <td>23 050</td>
                    <td>12 784 435</td>
                </tr>
                <tr>
                    <td><b>21/01</b></td>
                    <td>23 078</td>
                    <td>16 784 373</td>
                </tr>
                <tr>
                    <td><b>22/01</b></td>
                    <td>23 125</td>
                    <td>18 543 502</td>
                </tr>
                <tr>
                    <td><b>23/01</b></td>
                    <td>23 118</td>
                    <td>13 849 293</td>
                </tr>
            </table>
        </td>
        <td>
            =
        </td>
        <td>
            <table>
                <tr>
                    <td><b>date</b></td>
                    <td>price_x</td>
                    <td>volume_x</td>
                    <td>price_y</td>
                    <td>volume_y</td>
                </tr>
                <tr>
                    <td><b>19/01</b></td>
                    <td>NaN</td>
                    <td>NaN</td>
                    <td>23 064</td>
                    <td>15 067 894</td>
                </tr>
                <tr>
                    <td><b>20/01</b></td>
                    <td>14.56</td>
                    <td>135</td>
                    <td>23 050</td>
                    <td>12 784 435</td>
                </tr>
                <tr>
                    <td><b>21/01</b></td>
                    <td>15.23</td>
                    <td>280</td>
                    <td>23 078</td>
                    <td>16 784 373</td>
                </tr>
                <tr>
                    <td><b>22/01</b></td>
                    <td>15.07</td>
                    <td>73</td>
                    <td>23 125</td>
                    <td>18 543 502</td>
                </tr>
                <tr>
                    <td><b>23/01</b></td>
                    <td>15.56</td>
                    <td>160</td>
                    <td>23 125</td>
                    <td>18 543 502</td>
                </tr>
                <tr>
                    <td><b>24/01</b></td>
                    <td>17.85</td>
                    <td>1450</td>
                    <td>NaN</td>
                    <td>NaN</td>
                </tr>
            </table>
        </td>
    </tr>
</table>
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

заметим, что если мы просто склеим эти две таблицы горизонтально, то в одной строке получится информация за две разные даты, что, очевидно, неправильно. Для того, чтобы в итоговой таблице получилась правильная информация, надо. чтобы в обеих исходных таблицах присутствовал атрибут (столбец), который позволяет однозначно идентифицировать объект данных. В данном случае таким атрибутом выступит дата (выделена жирным). В реляционной алгебре это общее поле называется ключом, по которому происходит соединение таблиц. 

{% capture notice %}
Никогда не используйте простое склеивание (_append()_ или _concat()_ в _pandas_) для горизонтального объединения датасетов, только специальные реляционный оператор соединения, который в SQL называется JOIN. В том же _pandas_ его аналоги - _join()_ или _merge()_. 
{% endcapture %}
<div class="notice--danger">{{ notice | markdownify }}</div>

Существует много разных видов соединений таблиц по ключу - внутреннее, внешнее, левое, правое, перекрестное. Самое консервативное (в том смысле, что не удаляет никакую информацию) - внешнее полное соединение. В результирующую таблицу будет записана информация по каждому значению ключа, который присутствует хотя бы в одной таблице. Также в нее войдут все столбцы из обеих таблиц (кроме ключа, который как общая часть войдет в результат только один раз). В ячейки таблицы будет записана информация, либо специальный индикатор отсутствия значения. На примере выше показано как раз полное внешнее соединение двух таблиц. Обратите внимание, что в итоге получилось больше строк, чем в исходных таблицах, а также в некоторых ячейках стоит специальное значение "NaN". Затем эти пропуски можно уделить.

{% capture notice %}
Если в результате горизонтального или вертикального объединения данных получается большое количество пропусков, это может происходить от того, что в таблицах присутствует очень разная информация о разных объектах. То есть мало совпадений как по строкам, так и по столбцам. В таком случае стоит задуматься о том, имеет ли смысл вообще объединять такие таблицы. Или, может быть, если у вас есть еще много других таблиц, вы выбрали неоптимальный порядок их объединения.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

С помощью этих двух видов объединения данных можно практически любой разрозненное множество датасетов привести к форме из единой таблицы. Конечно, придется принять множество решений по сохранению или удалению части данных, выбрать оптимальный порядок объединения таблиц, поработать потом с отсутствующими значениями, которые появились в результате объединения.

Но самое сложное в процессе интеграции данных - следить за их соответствием и цельностью. В разных датасетах могут использоваться разные единицы измерения, разные обозначениях одних и тех же объектов, разные периоды измерений. Такие несоответствия - главная головная боль аналитика по данным. Чего стоит одна лишь работа с датами. В разных таблицах даты могут обозначаться тысячью разных способов. И если за этим не следить, то в итоге получится совершенно неприменимый на практике датасет. Существует множество типичных проблем, которые часто встречаются в данных, особенно в корпоративной среде. По опыту выполнения практических проектов можно выделить такие самые типичные примеры:

1. Разные обозначения и сокращения в данных. Например, номенклатура продукции может записываться в разных справочниках по-разному. Самое противное в этой проблеме, что ее приходится решать, руками записывая таблицы соответствия разных обозначений, что очень долго.
1. Разные группировки объектов. Например, в одном справочнике клиенты объединены по географическому принципу, а в другом - по суммарному чеку. В одной таблице могут быть сгруппированы объекты, которые в другой относятся к разным группам. В одной базе данных три наименее малочисленные группы объединены в "другое", в соседней - десять товаров, которые имеют менее 1% объема продаж - объединены в "остальные".
1. Иерархические справочники. Очень многие объекты группируются в разные иерархии. Например, та же номенклатура продукции может группироваться в группы, которые объединяются в категории и классы. Проблема в том, что не каких-то таблицах идет разбивка по конкретным товарам, в других - по категориям, в третьих - суммарные или средние значения по товарным группам. Наверное, хорошо для учета, но очень неудобно для аналитики и машинного обучения.
1. Разные периоды наблюдений. Как в примере выше - временные ряды могут часто не соотноситься по датам и времени измерений. В одной таблице измерения идут каждый день, в другой - только в рабочие дни. И приходится либо дополнять данные, либо выкидывать большой объем информации.
1. Разные временные базы измерений. Даже в пределах одной организации повсеместно встречается, что некоторые параметры измеряются посуточно, другие - суммарно за неделю, а третьи - раз в месяц. Понятно, что без специальной обработки их нельзя просто сводить вместе.
1. Отдельная проблема - всякие цифровые коды, индексы и классификаторы. Особенно - какие-нибудь 20-значные цифровые номера счетов, которые имеют определенную внутреннюю структуру. Эта структура обычно "очевидна" для специалистов, которые с ней работают, но совершенно секретна для аналитиков. Из-за этого приходится "парсить" численные значения, работать с ними как со строками. Но самое неприятное в том, что такие системы кодификации могут меняться со временем, что заставляет опять вводить сложные таблицы соответствия, причем это соответствие не всегда однозначно. Причем в разных таблицах опять же могут использоваться разные классификаторы, которые как-то "очевидно" между собой соотносятся. Кому интересно, можете познакомиться с такими понятиями, как ОГРН, ОКВЭД и ОКПО. А в специальных предметных отраслях подобных обозначений еще больше. 
1. Просто разные единицы измерений и обозначения дат. Это относительно простая проблема, потому что она решается автоматической конвертацией. Главное - вовремя ее заметить.

Это может касаться как технических деталей, так и сутевых, предметных аспектов. Например, при анализе эмоций в речи можно столкнуться с тем, что в разных наборах данных, которые можно собрать по данной теме, используется разный набор эмоций. То есть в разных датасетах разный набор значений целевой переменной. Это значит, что нельзя просто объединить их, нужно что-то делать с согласованностью значений. По сути, придется частично переразметить датасет. Конечно, это можно сделать автоматически, но важно просто не забыть про это, не пропустить этот этап. И ни в коем случае, нельзя предполагать, что в данных все хорошо и нет проблем. Это всегда надо проверять и тестировать.

{% capture notice %}
В этом разделе мы не рассматривали различные стратегии преобразования и парсинга данных которые требуются, если исходная информация существует в виде, сильно отличном от табличной или реляционной формы. Это уже предмет не аналитики данных, а общего программирования - преобразовать данные в той форме, которая необходима для анализа.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Именно за счет таких мелких деталей, интеграция данных считается самым сложным и трудозатратным этапом машинного обучения. Несмотря на то, что здесь не используется крутая математика, этот этап не требует огромных вычислительных мощностей, на практике, он занимает до 80% всего времени работы над проектом по машинному обучению. Кроме того на этом этапе возникают большинство ошибок и артефактов в данных, которые критично сказываются на эффективности моделей. Поэтому не нужно недооценивать важность и продолжительность этого этапа. На сбор и интеграцию данных не стоит жалеть усилия и экономить ресурсы. Всегда помните про принцип "мусор на входе - мусор на выходе", ведь объем и качество данных - самое важное для машинного обучения. 

{% capture notice %}
Выводы:
1. Интеграция данных - это процесс объединения данных из нескольких источников в единый датасет.
1. Объединение датасетов может происходить по горизонтали и по вертикали.
1. Вертикальное объединение датасетов - это по сути просто склеивание двух однотипных наборов данных в один.
1. Горизонтальное объединение датасетов происходит обязательно через аналог операции JOIN по ключу.
1. При объединении данных из разных источников необходимо особенно тщательно следить за обозначениями, соглашениями, датами.
1. Интеграция данных - самая трудная часть работы с машинным обучением.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Предварительная обработка данных

#### Описательный анализ данных (EDA)

После завершения сбора и интеграции данных, то есть когда у вас уже есть единый датасет, который планируется использовать как обучающую выборку, приходит пора заняться его анализом и обработкой. Конечно, отдельные элементы анализа данных стоит проводить и на этапе сбора информации из различных источников, чтобы обнаружить критичные проблемы в данных, из-за которых данный источник стоит вообще "забраковать". Но именно после получения готового датасета стоит заняться анализом и обработкой более системно, так как вы будете видеть всю доступную информацию и не придется повторять одни и те же процедуры на разных данных. 

В первую очередь надо определиться, зачем вообще нужен предварительный анализ данных? Разве мы не говорили, что если информация подходит по форме под определение "чистых данных", то ее можно использовать для моделирования? Да, но при сборе и интеграции данных можно обеспечить выполнение только основных требований чистых данных. Кроме этих обязательных, есть еще два условия, которые почти всегда стоит соблюсти. Это требование к отсутствию пропусков и требование к численному представлению данных. 

Вообще, данные, полученные из реального мира могут содержать всяческие проблемы и ошибки, которые почти всегда негативно сказываются на эффективности применяемых моделей машинного обучения. Кроме пропусков и неправильных типов данных могут присутствовать просто ошибки, опечатки, некорректные данные, аномальные, непоказательные объекты и еще много всего. Так что главная цель предварительной обработки и анализа данных в первую очередь состоит в том, чтобы еще больше очистить данные, устранить все возможные проблемы ("артефакты") в них.

Но механическая очистка данных - это еще не ве и не решение всех проблем. Мы уже несколько раз упоминали, что эффективная работа с данными невозможна без их глубокого понимания, погружения в предметную область. И поэтому вторая главная задача предварительного анализа данных - ближе познакомиться с этой самой информацией. Такой анализ может дать подсказки, какие именно методы предобработки лучше использовать. Ведь с теми же пропущенными значениями можно бороться несколькими принципиально разными способами. И вообще на этапе очистки данных приходится принимать множество потенциально важных для моделирования решений. Глубокий анализ данных позволяет выбрать наиболее эффективные методы как обработки данных, так и непосредственно моделирования.

![](/assets/images/ml_text/ml5-3.png "eda"){: .align-center style="width: 800px;"}

Даже такая простая информация, как диапазон измерения тех или иных атрибутов, которую в _pandas_ можно получить одной строкой может многое прояснить - шкалы измерения разных атрибутов, возможные ошибки в данных, значения, которые лежат вне разумных диапазонов, количество значений категориальных переменных. И это все может в будущем повлиять на методы обработки этих данных.

Еще раз подчеркнем, что в данной главе мы сосредоточимся на рассмотрении только главных этапов так называемого описательного анализа данных (exploratory data analysis, EDA) и предварительной обработки данных для их подготовки к использованию в моделях машинного обучения. Конечно, мы не можем даже обзорно рассмотреть все многообразие методов анализа и обработки данных. Поэтому далее рассмотрим шаги, которые являются критичными и самыми распространенными при анализе практически любых реальных данных. А при необходимости можно и даже нужно проводить более глубокий анализ данных, возможно, с привлечением статистических методов и критериев, проверки гипотез, математических доказательств и прочей тяжелой артиллерии.

Вообще, чем глубже проводить анализ данных, тем лучше. В большинстве случаем временные затраты на анализ используемой информации с лихвой компенсируются тем, что в процессе этого анализа можно обнаружить скрытые зависимости, тенденции, которые невозможно заметить при обычном беглом осмотре данных, а тем более при слепом использовании датасета в моделях. В конечном итоге подробный анализ данных приводит к получению более качественной информации. А в машинном обучении качество данных гораздо важнее продвинутости моделей и алгоритмов. Даже простые модели на хороших, подготовленных данных будут демонстрировать качественно боле высокую эффективность, чем самые крутые и глубокие нейросети на "грязных", неочищенных и нерелевантных датасетах.

В данном разделе мы будем рассматривать типичные задачи предварительного анализа и обработки данных отдельно. Но в реальности, это более взаимосвзяанный, итеративный процесс. Поэтому нет смысла рассматривать отдельно сначала анализ, затем обработку данных. Эти процессы переплетаются, ведь анализ может выявить проблемы в данных, которые нужно устранить, соответствующим образом преобразовав датасет. А преобразования данных зачастую требуют повторения определенных шагов анализа. Поэтому важно помнить про основные задачи этого этапа и решать их последовательно, перемежая исследование и изменение данных.

{% capture notice %}
Выводы:
1. Описательный анализ данных нужен для обнаружения проблем (артефактов) в данных и для выбора метода их устранения.
1. EDA также может дать информацию о структуре данных, шкалах измерения переменных, которые потом повлияют на методы обработки данных.
1. Также EDA может помочь при выборе признаков, выявлении зависимостей в данных.
1. EDA - это обзорный анализ данных именно при их подготовке к процессу машинного обучения.
1. При необходимости, можно провести более глубокий статистический анализ данных.
1. Анализ и обработку данных обычно производят параллельно, это не изолированные процессы.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Описание шкалы измерения атрибутов

Самый простой этап анализа данных - индивидуальное рассмотрение каждого атрибута в датасете. Для эффективной работы с информацией аналитик должен понимать смысл каждого столбца датасета, что он измеряет, какие значения может принимать, какие операции и преобразования с ним имеет смысл производить. Повторимся, что для моделирования важно понимать именно смысл используемых данных, не стоит ограничиваться формальным описанием. 

{% capture notice %}
Конечно, когда мы говорим про анализ каждого атрибута, имеется в виду каждый __значимый__ атрибут. В простых задачах, когда количество столбцов в датасете ограничено единицами или максимум, десятками, можно подробно поработать с каждой колонкой, провести небольшой статистический анализ абсолютно каждого атрибута. В реальных задачах часто количество атрибутов идет на сотни и тысячи, что делает такую индивидуальную работу невозможной. Конечно, надо исходить из конкретной задачи. Во многих проектах можно сильно сократить количество атрибутов, удалив лишние просто по смыслу. В других - атрибуты представляют собой какие-то однородные данные. Как, например, в картинках, где каждый пиксель - это отдельный признак. Конечно, не идет речь о том, чтобы анализировать каждый пиксель картинки отдельно. 
{% endcapture %}
<div class="notice--danger">{{ notice | markdownify }}</div>

Пре работе в конкретными атрибутами самое главное, что нужно понимать - это различные типы шкал. Шкала - это множество возможных значений переменной и совокупность операций и отношений, которые имеют смысл применительно к этой переменной. Типы шкал активно изучаются в таком разделе науки, как метрология. Общепринято различают четыре главных типа шкал по Стивенсу. Для анализа данных очень полезно охарактеризовать шкалу измерения каждого атрибута. А особенно - понять тип этой шкалы. Но для этого надо уметь различать эти типы на практике, исходя из смысла каждого атрибута как характеристики объекта предметной области. Поэтому рассмотрим основные типы и их отличительные черты. 

![](/assets/images/ml_text/ml5-2.png "Типы шкал"){: .align-center style="width: 800px;"}

Самое главное разделение, которое надо знать - это разница между численными и категориальными атрибутами (признаками). Категориальный признак может принимать только одно из конечного набора значений. Категориальные признаки - это как классы в задачах классификации. Они часто выражаются каким-то текстовым называнием, меткой, обозначением. Численные признаки, соответственно, могут принимать значения из некоторого непрерывного интервала и всегда могут быть выражены числом. Категориальные переменные еще называются иногда дискретными, качественными или неметрическими, а численные - непрерывными, количественными или метрическими.

Мы уже говорили про разницу между этими типами шкал, когда определяли задачу регрессии и классификации. Но категориальной или численной может быть не только целевая переменная, а любой атрибут или признак в данных. Это разделение так важно потому, что работа с категориальными и численными данными принципиально различается. В частности, все категориальные признаки перед началом машинного обучения нужно будет обязательно преобразовать в численные. 

{% capture notice %}
Важно не путать тип шкалы измерения атрибута и тип данных в языке программирования. Это схожие вещи, но если вы будете полагаться на анализ только по типу данных, вы будете совершать существенные ошибки в анализе. Численные атрибуты спокойно могут существовать в данных в виде строки, например просто потому, что где-то в процессе получения данных они так считались. Уже не будем говорить о любимых в некоторых сферах суммах прописью. А категориальные атрибуты вполне могут быть выражены числом (как, например, класс обслуживания в знаменитом датасете "Титаник").
{% endcapture %}
<div class="notice--danger">{{ notice | markdownify }}</div>

Пойдем немного дальше. Категориальные шкалы измерения признаков, в свою очередь, делятся на номинальные и порядковые шкалы. Номинальная шкала (ее еще называют шкалой наименований) - это такая, про каждые два значения которой можно только сказать, равны они или нет. То есть единственная операция, которая имеет смысл с переменной, выраженной в номинальной шкале - это операция проверки на равенство. Более математически выражаясь, можно сказать, что в номинальной шкале определено только отношение эквивалентности (тождества). Номинальные шкалы - это почти самый простой тип шкалы, это именно такие шкалы, про которые люди интуитивно думают, когда представляют себе категориальную переменную. 

Примером номинальной шкалы можно назвать различные объекты, которые можно распознавать на изображении, названия стран, марки телефонов, вообще любые названия - организаций, производителей, товаров, различные типы объектов. Кстати, не всегда категориальные или номинальные переменные обозначаются числом. Типичный пример - номера маршрутов общественного транспорта. Несмотря на то, что это именно номер, тип шкалы такой переменной - номинальная. 

Порядковые шкалы (их еще называют ординальные или ранговые) - это тоже категориальные переменные, то есть принимающие определенные значения из конечного набора, но между значениями которых есть отношение как тождества, так и порядка. Другими словами, про каждые два значения из этой шкалы можно сказать, какое из них больше (либо они равны друг другу). Еще говорят, что значения из этой шкалы можно ранжировать - выстроить по порядку. Еще раз обращаем ваше внимание, что тип шкалы - это не про какие-то конкретные значения, это про смысл данной переменной, признака, характеристики. Типичными примерами порядковой шкалы будут балльные оценки: удовлетворительно, хорошо, отлично; уровень образования - начальный, средний, высший. Несмотря на то, что значения этой характеристики обозначаются текстом, их можно сравнивать между собой. То есть можно сказать, что высшее образование - это больше, чем среднее, а среднее - это больше, чем начальное. Кстати, отношение порядка по определению транзитивно. Так что из этого следует, что высшее - больше, чем начальное. 

Среди категориальных шкал особое значение имеют так называемые бинарные переменные. Это такие атрибуты, которые могут принимать всего два значения. Они могут быть как порядковые, так и номинальные. Хотя отношение порядка в таких переменных носит довольно уловный характер. Типичный пример такой бинарной шкалы - пол. Почему эта шкала особенная? Именно в машинном обучении и анализе данных ее можно преобразовывать особым образом - представлять в виде булевой переменной со значениями 0 и 1. Причем порядок этих значений не играет роли. Вспомните бинарную классификацию и то, какую особую роль она играет. Бинарная шкала - это действительно самый простой тип шкалы.

Ключевое различие между порядковыми (ординальными) признаками и номинальными в том, как их можно преобразовывать в численную форму. Делов том, что номинальные признаки нельзя просто поименовать числами, как многие делают в анализе данных. Это большая ошибка, так как это преобразование привносит в данные искусственный порядок, которого в них не было (по определению номинальной шкалы). Допустим, у нас в датасете есть колонка "производитель телефона". И для того, чтобы преобразовать эту категориальную переменную в число мы просто обозначили разные значения числами, например "Apple" - 0, "Samsung" - 1, "Sony" - 2 и так далее. После такого преобразования модель будет воспринимать эту переменную как численную. А это значит, что модель может в своих расчетах полагаться на то, что Sony - это в два раза больше, чем Samsung, что, очевидно, абсурд. Как именно преобразовывать такие переменные мы расскажем в следующих главах.

{% capture notice %}
Здесь надо отметить, что часто к какой шкале отнести переменную зависит и от интерпретации задачи. Опять же, здесь надо исходить из смысла, а не формы. В том же примере с производителями телефонов можно придумать пример, в котором нам важна, например, рыночная доля каждого производителя. Тогда между ними действительно можно выстроить некоторый порядок. Так и в других примерах, в зависимости от нашего понимания и контекста задачи одна и та же характеристика объекта может быть отнесена к разным типам шкал. Но, надо признать, это довольно редкий и экзотический случай. Обычно тип шкалы полностью очевиден из смысла данной переменной.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Переходим к численным шкалам. Различие между конкретными видами численных шкал гораздо менее важно для машинного обучения, чем между номинальной и порядковой. Но для полноты изложения, приведем и их характеристики. Тем более, что они все-так могут пригодиться на практике. Среди численных шкал выделяют интервальную и абсолютную шкалу. Интервальная шкала (она же шкала разностей) - это такая, про два значения из которой всегда можно сказать, на сколько одно больше другого (кроме сравнения и эквивалентности, как в предыдущих шкалах). То есть в ней имеют смысл разницы между значениями. Типичный пример - даты. Для данной шкалы имеют смысл такие преобразования, как сдвиги. То есть можно, например, из даты рождения получить возраст человека. А вот умножать даты и возраст некорректно.

Наконец, абсолютная шкала (ее также называют шкалой отношений) - это такая, для любых двух значений которой можно еще сказать, во сколько раз одно значение больше другого. Опять же, именно эту шкалу интуитивно представляют, когда говорят о численных переменных. Это свойство шкале придает наличие абсолютного нуля - точки отсчета. Данная шкала допускает, кроме вех предыдущих еще и умножение на константу. Именно по абсолютной шкале измеряются все физические величины - вес, рост, длина, напряжение, а также стоимость, цена. Вообще, абсолютная шкала очень распространена. 

Надо отметить еще один специальный тип шкалы, с которым надо работать особо, хотя формально, по Стивенсону, он относится к абсолютной шкале. Очень часто в экономических и физических задачах встречаются величины, которые могут принимать только положительные значения, от нуля до бесконечности и при этом их значения покрывают несколько порядков величины. У таких переменных имеет очень небольшой смысл разница между значениями, чаще нам важнее именно отношение. За счет того, что абсолютные значения в таких переменных очень сильно разнятся по величине, на практике очень удобно рассматривать логарифм этих значений. Поэтому такую шкалу будем называть логарифмической. Мы уже сталкивались с такой проблемой, когда рассматривали метрики эффективности регрессии.

Определение типа шкалы это важная часть работы с каждым отдельным атрибутом датасета. Но шкала измерения переменной не ограничивается только типом. Разные переменные могут измеряться по разным шкалам, даже относящимся к одному типу. Поэтому кроме типа необходимо рассмотреть еще несколько ключевых характеристик шкал. Во-первых, это так называемая мера центрального элемента или, простыми словами, среднее значение данной переменной. Проблема в том, что по разным типам шкал центральный элемент нужно определять разными способами. Для номинальных шкал единственное, что можно определить - моду, то есть такое значение, которое встречается в выборке чаще других. Для порядковой шкалы можно посчитать медиану - значение, больше которого и меньше которого одинаковое количество объектов в датасете. По шкале отношений самой адекватной мерой центрального элемента будет простое среднее арифметическое. Оно же часто используется и для абсолютной шкалы, но если мы имеем дело с логарифмической переменной, то более грамотно будет воспользоваться средним геометрическим.

Кроме центрального элемента интерес представляет в целом исследовать, как объекты выборки распределяются по разным значениям из шкалы данной переменной. Для категориальных признаков можно узнать все значения (их же конечное количество) то, сколько объектов датасета имеет каждое конкретное значение. Это и есть дискретное распределение (причем эмпирическое, ведь мы говорим о реальных данных). А для численных шкал очень полезно узнать диапазон значений (минимальное и максимальное значение) и то, какие интервалы внутри него более "популярны", а какие менее. Для этого можно строить таблицы, процентили и квартили. Но самым удобным и наглядным инструментом будет гистограмма. Она покажет общий вид распределения объектов выборки по значениям любого признака - как категориального, так и численного. НО надо помнить, что для переменных, в которых присутствует порядок (то есть с порядковых и дальше) значения в гистограмме было бы логично и более наглядно упорядочить именно в этом, естественном порядке. Что, в общем, справедливо и для других инструментов анализа - и графических и табличных.

Даже простой анализ шкалы распределения может показать очень многое -  можно выявить аномалии в данных, значения вне разумного диапазона, банальные опечатки или артефакты преобразования данных (в программном коде, который осуществляет сбор, интеграцию и преобразование данных тоже могут содержаться ошибки). Можно сделать предположение о виде распределения каждой переменной - равномерное ли оно, похоже ли на нормальное, экспоненциальное или какое-то другое из известных статистических распределений. Особенно странно смотрятся мультимодальные распределения, а также необъяснимые "провалы" на гистограммах. Все такие аномалии и странности нужно попытаться проинтерпретировать, объяснить. Самые странные артефакты - ошибки, опечатки, "странные" единичные значения можно попробовать удалить из датасета. Но опять же, в каждом конкретном случае надо руководствоваться пониманием предметной облатси и контекста задачи. 

{% capture notice %}
Выводы:
1. Шкалы атрибутов показывают, какие значения может принимать этот атрибут и как его можно интерпретировать и сравнивать.
1. Все атрибуты подразделяются на численные (непрерывные) и категориальные (дискретные). 
1. Категориальные переменные потом придется преобразовать в численные.
1. Номинальная шкала - это признак, для которого имеет смысл только равенство. Пример - метка класса.
1. Порядковая шкала - это когда наряду с равенством мы можем выстроить значения по порядку, который имеет смысл. Пример - класс обслуживания.
1. Интервалая шкала - это когда еще имеет смысл говорить о разнице между двумя значениями. Пример - даты.
1. Абсолютная шкала - это когда еще можно говорить о том, во сколько раз одно значение больше другого. Пример - сумма денег.
1. Особенные типы шкал - бинарная и логарифмическая.
1. Кроме типа шкала характеризуется диапазоном или набором значений, мерой центрального элемента и разброса, распределением.
1. Самый полезный инструмент для характеристики шкалы - гистограммы.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Заполнение отсутствующих значений

![](/assets/images/ml_text/ml5-4.png "отсутствующие значения"){: .align-center style="width: 800px;"}

![как выглядят пропуски](https://vitalflux.com/wp-content/uploads/2020/07/Screenshot-2020-07-23-at-12.07.27-PM.png "как выглядят пропуски"){: .align-center style="width: 800px;"}
Источник: [Data Analytics](https://www.google.com/url?sa=i&url=https%3A%2F%2Fvitalflux.com%2Fpandas-fillna-method-imputing-missing-values%2F&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![заполнение средним](https://miro.medium.com/max/1400/1*A7RpRo96poWAnyFcPoR_Jg.png "заполнение средним"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fa-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. При выборе стратегии борьбы с пропусками в данных следует проанализировать, сколько пропусков и где они расположены.
1. Если по одному из признаков большинство значений пропущено, можно задуматься об удалении этого признака из датасета.
1. Если наоборот, по одному объекту много неизвестных атрибутов, можно удалить объект.
1. Следует следить, чтобы от датасета что-то осталось после массового удаления.
1. Самый простой способ заполнить пропуски - заполнить их средним значением, но это сильно искажает форму распределения признака.
1. Зачастую можно считать групповое среднее, более "индивидуальную" оценку атрибута данного объекта, с учетом значений других атрибутов.
1. Иногда применяют заполнение случайным значением. Его лучше всего генерировать из распределения данного признака.
1. Самое грамотное решение - введение нового признака или заполнение специальным значением (по категориальным признакам).
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Оценка влияния атрибутов на целевую переменную

В случае задачи обучения с учителем весьма полезно оценить форму совместного распределения целевой переменной с каждым признаком в отдельности. Это позволяет сделать первичное эмпирическое предположение о влиянии каждого фактора на результирующую переменную в случае обнаружения распределения, сильно отличающегося от равномерного.

Основные задачи:

- Выдвижение гипотез об относительной важности факторов;
- Обоснование введения суррогатных факторов.

![](/assets/images/ml_text/ml5-8.png "непрерывная-дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-9.png "дискретная дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-10.png "дискретная бинарная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-11.png "табличная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-16.png "ящик с усами"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-15.png "дискретная-непрерывная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-12.png "сложная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-13.png "таблица с гистограммой"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-14.png "фасетная"){: .align-center style="width: 800px;"}

По аналогии с предыдущим этапом, довольно полезную информацию дает визуальная оценка совместного распределения каждой пары признаков и целевой переменной. Конечно, обратной стороной является квадратичная зависимости количества пар признаков, что делает практически неприменимым исчерпывающий анализ по всем возможным парам при более-менее большом количестве атрибутов в наборе данных. Поэтому исследуются чаще всего только те пары, которые могут представлять интерес, как по соображениям предметной области, так и по результатам анализа индивидуального влияния признаков.

Основные задачи - те же, что и на предыдущем этапе.

![](/assets/images/ml_text/ml5-17.png "три признака"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-18.png "три признака ирисы"){: .align-center style="width: 800px;"}

![feature importance](https://miro.medium.com/max/1400/0*vKRFyD57QMHC1WAp "feature importance"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285&psig=AOvVaw25OTcH_aslwWvq9xGRX-9S&ust=1670433370537000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJCIuK-_5fsCFQAAAAAdAAAAABAc).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Очень полезно оценить влияние каждого атрибута на целевую переменную.
1. Для этого обычно строят совместное распределение атрибута и целевой переменной. Какой тип визуализации использовать - зависит от шкал.
1. Если обе переменные численные - строят диаграмму рассеяния.
1. Если обе переменные категориальные - можно строить гистограммы с несколькими столбцами или таблицы (часто раскрашивают).
1. Если шкалы разные, то обычно строят график с несколькими линиями.
1. При необходимости можно строить графики совместного распределения двух атрибутов и целевой переменной. Но они уже более сложные.
1. Этот этап может дать информацию о том, какие атрибуты важны, а какие можно и удалить из модели.
1. Можно строить простые модели, чтобы анализировать feature importance.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Преобразование категориальных атрибутов

![Labelencoder](https://miro.medium.com/max/386/1*Yp6r7m82IoSnnZDPpDpYNw.png "Labelencoder"){: .align-center style="width: 800px;"}
Источник: [Medium](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40chexki_%2Fusing-label-encoder-on-unbalanced-categorical-data-in-machine-learning-using-python-435f521323b1&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![labelencoder done right](https://miro.medium.com/max/588/1*KdCvKnI9ATVPiozmuRindA.png "labelencoder done right"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

![one hot encoder](https://codecamp.ru/content/images/2021/09/oneHot1.png "one hot encoder"){: .align-center style="width: 800px;"}
Источник: [Кодкамп](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.codecamp.ru%2Fblog%2Fone-hot-encoding-in-python%2F&psig=AOvVaw1SjP5tmHbyYdNMFaBhJ2OR&ust=1670432674003000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCODul-O85fsCFQAAAAAdAAAAABAN).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Категориальные признаки часто выражаются строковыми данными и не подходят для использования в машинном обучении, их преобразуют в численные.
1. Самый простой кодировщик - LabelEncoder - просто нумерует все значения категориального признака.
1. Он вводит порядок в значения категорий, которого раньше не было, поэтому можно исказить результаты обучения.
1. Исключение - бинарные признаки, их можно кодировать как 0 и 1.
1. Более продвинутый кодировщик - OneHotEncoder - преобразует один признак во множество.
1. Новые признаки соответствуют значениям исходного и кодируются бинарно. Этот способ более универсален и рекомендуется применять по умолчанию.
1. В принципе, один из получившихся признаков можно удалить, но обычно никто не заморачивается.
1. Да, из одного признака может получиться тысяча. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Воспроизводимость преобразования данных

{% capture notice %}
Выводы:
1. Алгоритм, по которому выполнялось преобразование обучающей выборки необходимо сохранить, так как именно такое же преобразование нужно будет проделать над тестовой.
1. Да, преобразование данных обычно проводят уже после разделения выборки на обучающую и тестовую.
1. При параметрических преобразованиях все значения параметров подбираются именно по обучающей выборке.
1. Это может означать, что в тестовой после нормализации признак может быть больше 1 (или меньше 0), это нормально, так и надо.
1. Если используется преобразование данных, то модель будет работать на преобразованных данных, а из реального мира будут приходить исходные. 
1. Обычно преобразование данных оформляется как функция или (в sklearn) как обучаемая модель.
1. Преобразование данных используется в составе конвейеров.
1. В процессе исследования бывает необходимо попробовать разные методы преобразования данных в комбинации с разными моделями обучения.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>


<!--  -->


<!-- ### Описательный анализ данных (EDA)

![](/assets/images/ml_text/ml5-3.png "eda"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Описательный анализ данных нужен для обнаружения проблем (артефактов) в данных и для выбора метода их устранения.
1. EDA также может дать информацию о структуре данных, шкалах измерения переменных, которые потом повлияют на методы обработки данных.
1. Также EDA может помочь при выборе признаков, выявлении зависимостей в данных.
1. EDA - это обзорный анализ данных именно при их подготовке к процессу машинного обучения.
1. При необходимости, можно провести более глубокий статистический анализ данных.
1. Анализ и обработку данных обычно производят параллельно, это не изолированные процессы.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Анализ репрезентативности датасета

![](/assets/images/ml_text/ml5-24.png "репрезентативность"){: .align-center style="width: 800px;"}

Не самым распространенным, но, несомненно, полезным этапом описательного анализа данных является анализ репрезентативности выборки. Конечно, не в каждой задаче исследователь имеет доступ к информации о генеральной совокупности объектов описываемой предметной области, однако, в случае наличия информации, даже простой подсчет, какой процент ГС описывает обучающий набор данных может дать представление о потенциальной обобщающей возможности модели машинного обучения.
Кроме того, на данном этапе может выполняться проверка адекватности распределения признаков теоретическим или эмпирическим экспертным знаниям о предметной области. 

Основные задачи:

- Анализ репрезентативности выборки;
- Оценка целесообразности моделирования.

{% capture notice %}
Выводы:
1. Для машинного обучения важно, чтобы датасет правильно отражал генеральную совокупность. Это дает модели обобщающую способность.
1. Анализ соотношения выборки и генеральной совокупность не всегда возможно провести, но когда можно - нужно это делать.
1. Самая простая метрика - процент ГС, который отражен в выборке.
1. Важно понимать, как были собраны данные, по какому принципу они отбирались. Лучше - случайно.
1. Может быть проведена проверка адекватности распределения признаков экспертным знаниям в области.
1. На этом этапе принимается общее решение о целесообразности моделирования.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Описание шкалы измерения атрибутов

![](/assets/images/ml_text/ml5-2.png "Типы шкал"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Шкалы атрибутов показывают, какие значения может принимать этот атрибут и как его можно интерпретировать и сравнивать.
1. Все атрибуты подразделяются на численные (непрерывные) и категориальные (дискретные). 
1. Категориальные переменные потом придется преобразовать в численные.
1. Номинальная шкала - это признак, для которого имеет смысл только равенство. Пример - метка класса.
1. Ординальная шкала - это когда наряду с равенством мы можем выстроить значения по порядку, который имеет смысл. Пример - класс обслуживания.
1. Ординальные признаки можно преобразовывать методом LabelEncoder. Номинальные - только OneHotEncoder.
1. Интервалая шкала - это когда еще имеет смысл говорить о разнице между двумя значениями. Пример - даты.
1. Абсолютная шкала - это когда еще можно говорить о том, во сколько раз одно значение больше другого. Пример - сумма денег.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Визуализация распределения атрибутов

![](/assets/images/ml_text/ml5-5.png "визуализация распределения признака"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-6.png "визуализация нестандартного признака"){: .align-center style="width: 800px;"}

Также на начальном этапе зачастую строят индивидуальное эмпирическое распределение каждого признака, что позволяет выдвинуть предварительную гипотезу о виде распределения соответствующей переменной в генеральной совокупности. Вид распределения может оказать влияние на выбор метода нормализации данных (по среднему, по разбросу или стандартизация), выявить проблему смещенных классов, особенно острую в распределении по целевой переменной, индицировать системные ошибки выборки. Анализ индивидуального распределения может также помочь выявить аномальные выбросы, ошибки измерений, или опечатки в данных.

{% capture notice %}
Выводы:
1. Оценка эмпирического распределения атрибутов дает ценную информацию, которая определяет дальнейшую работу с этим атрибутом.
1. Можно использовать статистические методы, но обычно достаточно визуально оценить распределение по графику.
1. Категориальные переменные обычно изображают на графике, или списком в порядке убывания количества объектов.
1. Для численных переменных обычно строят график плотности распределения, хотя иногда - тоже гистограмму.
1. Для категориальных переменных оценивают долю каждого класса в датасете, сбалансированность распределения, моду.
1. Для численных переменных оценивают минимальное и максимальное значение, меры центрального элемента и разброса.
1. Для численных переменные еще оценивают вид распределения - равномерное, нормальное, логнормальное, экспоненциальное и т.д.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Проблема несбалансированных классов

![](/assets/images/ml_text/ml5-7.png "это еще не дисбаланс"){: .align-center style="width: 800px;"}

![Дисбаланс классов](https://miro.medium.com/max/425/1*GkhP_fliGHcijoiyp0I4Wg.png "Дисбаланс классов"){: .align-center style="width: 800px;"}
Источник: [analyticsvidhya.com](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2020%2F07%2F10-techniques-to-deal-with-class-imbalance-in-machine-learning%2F&psig=AOvVaw3luN7CCjqQGkXMf93CNqN3&ust=1670430010475000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCLCIp-2y5fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

![Множественный дисбаланс](https://www.researchgate.net/publication/344485627/figure/fig1/AS:943477185585154@1601953898776/a-Class-imbalanced-data-distribution-of-EGTEA-dataset-b-Class-wise-classification.png "Множественный дисбаланс"){: .align-center style="width: 800px;"}
Источник: [ResearchGate](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2Fa-Class-imbalanced-data-distribution-of-EGTEA-dataset-b-Class-wise-classification_fig1_344485627&psig=AOvVaw3luN7CCjqQGkXMf93CNqN3&ust=1670430010475000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCLCIp-2y5fsCFQAAAAAdAAAAABAW).
{: style="text-align: center; font-size:0.7em;"}

![К чему приводит дисбаланс](https://i.stack.imgur.com/boL50.png "К чему приводит дисбаланс"){: .align-center style="width: 800px;"}
Источник: [DSSE](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdatascience.stackexchange.com%2Fquestions%2F72958%2Fwhat-does-the-classification-report-interpret-class-1-indicates-abnormal-data&psig=AOvVaw0r0neS3g1ImN1IgVY085er&ust=1670430330798000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCICs9oW05fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Сбалансированность классов - это равномерность распределения целевой переменной в задачах классификации.
1. Сильно несбалансированные классы плохо сказываются на эффективности обучения, так как модель подстраивается под мажоритарный класс.
1. Самый простой способ устранения - удаление случайной части объектов мажоритарного класса, но это приводит к кратному уменьшения датасета.
1. Можно попробовать добавить объектов миноритарных классов, но это не всегда возможно.
1. С несбалансированными классами неплохо справляются иерархические классификаторы.
1. Можно попробовать модифицировать алгоритм обучения, придав классам веса.
1. Если ничего не помогает, возможно следует пересмотреть постановку задачи, разбить мажоритарный класс, переформулировать проблему.
1. Нужно проводить анализ ошибок
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Оценка влияния атрибутов на целевую переменную

В случае задачи обучения с учителем весьма полезно оценить форму совместного распределения целевой переменной с каждым признаком в отдельности. Это позволяет сделать первичное эмпирическое предположение о влиянии каждого фактора на результирующую переменную в случае обнаружения распределения, сильно отличающегося от равномерного.

Основные задачи:

- Выдвижение гипотез об относительной важности факторов;
- Обоснование введения суррогатных факторов.

![](/assets/images/ml_text/ml5-8.png "непрерывная-дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-9.png "дискретная дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-10.png "дискретная бинарная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-11.png "табличная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-16.png "ящик с усами"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-15.png "дискретная-непрерывная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-12.png "сложная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-13.png "таблица с гистограммой"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-14.png "фасетная"){: .align-center style="width: 800px;"}

По аналогии с предыдущим этапом, довольно полезную информацию дает визуальная оценка совместного распределения каждой пары признаков и целевой переменной. Конечно, обратной стороной является квадратичная зависимости количества пар признаков, что делает практически неприменимым исчерпывающий анализ по всем возможным парам при более-менее большом количестве атрибутов в наборе данных. Поэтому исследуются чаще всего только те пары, которые могут представлять интерес, как по соображениям предметной области, так и по результатам анализа индивидуального влияния признаков.

Основные задачи - те же, что и на предыдущем этапе.

![](/assets/images/ml_text/ml5-17.png "три признака"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-18.png "три признака ирисы"){: .align-center style="width: 800px;"}

![feature importance](https://miro.medium.com/max/1400/0*vKRFyD57QMHC1WAp "feature importance"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285&psig=AOvVaw25OTcH_aslwWvq9xGRX-9S&ust=1670433370537000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJCIuK-_5fsCFQAAAAAdAAAAABAc).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Очень полезно оценить влияние каждого атрибута на целевую переменную.
1. Для этого обычно строят совместное распределение атрибута и целевой переменной. Какой тип визуализации использовать - зависит от шкал.
1. Если обе переменные численные - строят диаграмму рассеяния.
1. Если обе переменные категориальные - можно строить гистограммы с несколькими столбцами или таблицы (часто раскрашивают).
1. Если шкалы разные, то обычно строят график с несколькими линиями.
1. При необходимости можно строить графики совместного распределения двух атрибутов и целевой переменной. Но они уже более сложные.
1. Этот этап может дать информацию о том, какие атрибуты важны, а какие можно и удалить из модели.
1. Можно строить простые модели, чтобы анализировать feature importance.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Построение корреляционной матрицы

Построение корреляционной матрицы дает очень наглядное и полное представление о влиянии каждого атрибута как на целевую переменную, так и на другие факторы, поэтому многие исследователи используют ее как завершающий этап описательного анализа данных. Однако, недостатком корреляционной матрицы можно является учет только факторов, выраженных в числовых шкалах.
Корреляционная матрица прекрасно выявляет мультиколлинеарность факторов и является первой диагностикой для исключения факторов из дальнейшего моделирования.

Основные задачи:

- Выдвижение гипотезы об относительной важности факторов
- Обнаружение мультиколлинеарности факторов

![](/assets/images/ml_text/ml5-20.png "маленькая коррелограмма"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-21.png "большая коррелограмма"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-22.png "матрица совместных распределений"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-23.png "большая матрица"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Коррелограмма - это популярный инструмент визуализации, он дает много информации, а построить его просто.
1. Корреляционная матрица дает представление о степени влияния каждого фактор на каждый и на целевую переменную.
1. Корреляционная матрица по своей природе симметрична относительно главной диагонали, в которой стоят единицы.
1. Коррелограмма может также дать информацию о мультиколлинеарности атрибутов.
1. Стоит обращать внимание как на положительные, так и на отрицательные корреляции.
1. Коррелограмма показывает только линейные зависимости.
1. По необходимости еще строится матрица совместных распределений.
1. Можно проводить и до и после преобразования данных, но если атрибутов слишком много, матрица получится перегруженной.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Обнаружение аномальных объектов в датасете

Задача обнаружения аномалий сама по себе представляет собой область машинного обучения. Методы и инструменты данной сферы иногда применяются для предварительного анализа данных в целях идентификации выбросов, выбивающихся из общего ряда наблюдений значений конкретного признака. Несмотря на то, что задача обнаружения выбросов, несомненно, полезна для анализа данных, использование такого сложного и затратного инструмента редко может быть оправдано. 

Основные задачи:

- Обнаружение аномальных объектов, потенциально ошибок в данных, потенциально нерелевантных членов выборки.

![](/assets/images/ml_text/ml5-25.png "без аномалий"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-26.png "может быть аномалия"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-27.png "выброс"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Аномальные объекты - это те, которые по своим значениям сильно отличаются от большинства.
1. Часто аномалии означают ошибки в данных, опечатки, значения вне разумного диапазона для атрибута.
1. Выбросы (единичные значения атрибута, далеко отстоящие от всех соседей) - почти всегда свидетельствуют об ошибках в данных.
1. Аномалии можно увидеть на графиках, но если атрибутов слишком много, не все аномалии могут быть выявлены.
1. Обнаружение аномалий - сама по себе задача машинного обучения, которая может выявить аномалии не только по одному измерению, а по всем.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Очистка и преобразование данных

#### Удаление лишних признаков (feature selection)

{% capture notice %}
Выводы:
1. Выбор признаков - важная часть преобразования данных, ведь чем точнее мы определим необходимую для моделирования информацию, тем эффективнее будет проходить обучение.
1. Не стоит оставлять в датасете ненужные признаки - это повышает вариативность моделей и может приводить к переобучению.
1. Для выбора признаком часто используют выводы из EDA, обучение простых моделей (DT, RF) или здравый смысл.
1. Выбор признаков можно автоматизировать - применять алгоритм очередного удаления или добавления признаков.
1. Выбор признаков менее актуален, если обучаются нейронные сети, они способны сами отбирать признаки.
1. Регуляризация обычно работает лучше отбора признаков.
1. Совсем лишние данные все равно надо убирать.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Удаление непоказательных объектов

{% capture notice %}
Выводы:
1. Аномальные объекты часто удаляются из датасета, чтобы не искажать результаты обучения.
1. Если можно исправить выброс, можно попытаться это сделать.
1. Следует анализировать, показателен ли объект для предметной области. Если нет - можно смело удалять.
1. После удаления множества объектов может понадобиться повторить анализ сбалансированности классов.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Заполнение отсутствующих значений

![](/assets/images/ml_text/ml5-4.png "отсутствующие значения"){: .align-center style="width: 800px;"}

![как выглядят пропуски](https://vitalflux.com/wp-content/uploads/2020/07/Screenshot-2020-07-23-at-12.07.27-PM.png "как выглядят пропуски"){: .align-center style="width: 800px;"}
Источник: [Data Analytics](https://www.google.com/url?sa=i&url=https%3A%2F%2Fvitalflux.com%2Fpandas-fillna-method-imputing-missing-values%2F&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![заполнение средним](https://miro.medium.com/max/1400/1*A7RpRo96poWAnyFcPoR_Jg.png "заполнение средним"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fa-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. При выборе стратегии борьбы с пропусками в данных следует проанализировать, сколько пропусков и где они расположены.
1. Если по одному из признаков большинство значений пропущено, можно задуматься об удалении этого признака из датасета.
1. Если наоборот, по одному объекту много неизвестных атрибутов, можно удалить объект.
1. Следует следить, чтобы от датасета что-то осталось после массового удаления.
1. Самый простой способ заполнить пропуски - заполнить их средним значением, но это сильно искажает форму распределения признака.
1. Зачастую можно считать групповое среднее, более "индивидуальную" оценку атрибута данного объекта, с учетом значений других атрибутов.
1. Иногда применяют заполнение случайным значением. Его лучше всего генерировать из распределения данного признака.
1. Самое грамотное решение - введение нового признака или заполнение специальным значением (по категориальным признакам).
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Создание суррогатных признаков (feature engineering)

{% capture notice %}
Выводы:
1. Инжиниринг признаков - сложная и творческа работа, но зачастую именно в ней кроется секрет существенного повышения эффективности моделей.
1. Суррогатные признаки - это как вычислимые поля в базах данных, они создаются на основе уже присутствующих в датасете атрибутов.
1. При создании новых признаков нужно руководствоваться здравым смыслом и знанием предметной области.
1. Зачастую новые признаки - это часть существующих, которые потом удаляются.
1. Например, из фамилии можно получить список родственников, из даты рождения - возраст и так далее.
1. После введения суррогатных признаков следует повторить EDA, хотя бы частично.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Преобразование численных атрибутов в категориальные

Довольно часто исследователи проводят анализ количества объектов в обучающем наборе данных, удовлетворяющим каким-либо условиям, вытекающим из описания предметной области, например, попадающим в определенный промежуток значений, обладающих уникальным набором характеристик, соответствующих конкретным значениям количественной характеристики (чаще всего, 0 или 1). Такой анализ позволяет выявить и исключить из выборки нерелевантные объекты.

Основные задачи:

- Выявление неочевидных кластеров объектов предметной области;
- Выявление непоказательных объектов в наборе данных.

![](/assets/images/ml_text/ml5-19.png "группировка в титанике"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Иногда бывает целесообразно сгруппировать объекты датасета по значению какого-то признака и заменить его названием группы.
1. В таком случае, мы удаляет часть информации из модели, но это может быть лишняя вариативность.
1. Показательный пример - группы населения по возрасту.
1. Такое нужно делать, только если есть уверенность, что объекты внутри группы одинаково относятся к целевой переменной.
1. Границы групп выбирают вручную, от этого многое зависит.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Преобразование категориальных атрибутов

![Labelencoder](https://miro.medium.com/max/386/1*Yp6r7m82IoSnnZDPpDpYNw.png "Labelencoder"){: .align-center style="width: 800px;"}
Источник: [Medium](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40chexki_%2Fusing-label-encoder-on-unbalanced-categorical-data-in-machine-learning-using-python-435f521323b1&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![labelencoder done right](https://miro.medium.com/max/588/1*KdCvKnI9ATVPiozmuRindA.png "labelencoder done right"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

![one hot encoder](https://codecamp.ru/content/images/2021/09/oneHot1.png "one hot encoder"){: .align-center style="width: 800px;"}
Источник: [Кодкамп](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.codecamp.ru%2Fblog%2Fone-hot-encoding-in-python%2F&psig=AOvVaw1SjP5tmHbyYdNMFaBhJ2OR&ust=1670432674003000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCODul-O85fsCFQAAAAAdAAAAABAN).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Категориальные признаки часто выражаются строковыми данными и не подходят для использования в машинном обучении, их преобразуют в численные.
1. Самый простой кодировщик - LabelEncoder - просто нумерует все значения категориального признака.
1. Он вводит порядок в значения категорий, которого раньше не было, поэтому можно исказить результаты обучения.
1. Исключение - бинарные признаки, их можно кодировать как 0 и 1.
1. Более продвинутый кодировщик - OneHotEncoder - преобразует один признак во множество.
1. Новые признаки соответствуют значениям исходного и кодируются бинарно. Этот способ более универсален и рекомендуется применять по умолчанию.
1. В принципе, один из получившихся признаков можно удалить, но обычно никто не заморачивается.
1. Да, из одного признака может получиться тысяча. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Нормализация и решкалирование признаков

Минимаксная нормализация - это изменение входных данных по следующей формуле:

{% capture block %}
$$ x' = \frac{x - x_{min}}{x_{max} - x_{min}} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

После преобразования все значения будут лежать в диапазоне $x \in [0; 1]$. 

Z-оценки или стандартизация производится по формуле:

{% capture block %}
$$ x' = \frac{x - M[x]}{\sigma_x} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

В таком случае данный признак приводится к стандартному распределению, то есть такому, у которого среднее 0, а дисперсия - 1. 

![scaler](https://miro.medium.com/max/1200/1*UPLv3kNw9JTtNabr70dQDQ.png "scaler"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-and-why-to-standardize-your-data-996926c2c832&psig=AOvVaw0vaCY5M7N0Ieg06Xjz_Et6&ust=1670432861032000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCTtLy95fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Нормализация признаков нужна для ускорения обучения и сходимости градиентного спуска.
1. Во многих реализациях моделей нормализация уже встроена и применяется по умолчанию.
1. Основная идея нормализации - это сделать так, чтобы все признаки измерялись по одной шкале, то есть лежали в одних пределах.
1. Обычно для этого выбирают шкалу от 0 до 1. Тогда каждое значение нужно разделить на максимальное.
1. Еще применяют стандартизацию - приведение к стандартному распределению.
1. Нормализация - это параметрическое преобразование, нужно запоминать, на что делили или что отнимали.
1. О отдельных случая применяются более крутые алгоритмы решкалирования с автоматическим устранением выбросов.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Краткий сценарий анализа данных

{% capture notice %}
Выводы:
1. Интеграция и очистка данных.
1. Подробное описание каждого признака, шкалы, вида распределения.
1. Исследование отсутствующих значений, заполнение или удаление.
1. Описание вида распределения каждого значимого признака и целевой переменной.
1. Преобразования категориальных признаков в численные.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Подробный сценарий анализа данных

{% capture notice %}
Выводы:
1. Оценка источников и объемов данных
1. Анализ репрезентативности и однородности разделения выборки.
1. Гипотезы о виде распределения каждого признака и выявление аномалий, визуализация.
1. Выявление и исправление несбалансированности классов.
1. Корреляционная матрица, выявление мультиколлинеарности, важность признаков.
1. Отбор признаков, инжиниринг признаков, возможная группировка значений.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Воспроизводимость преобразования данных

{% capture notice %}
Выводы:
1. Алгоритм, по которому выполнялось преобразование обучающей выборки необходимо сохранить, так как именно такое же преобразование нужно будет проделать над тестовой.
1. Да, преобразование данных обычно проводят уже после разделения выборки на обучающую и тестовую.
1. При параметрических преобразованиях все значения параметров подбираются именно по обучающей выборке.
1. Это может означать, что в тестовой после нормализации признак может быть больше 1 (или меньше 0), это нормально, так и надо.
1. Если используется преобразование данных, то модель будет работать на преобразованных данных, а из реального мира будут приходить исходные. 
1. Обычно преобразование данных оформляется как функция или (в sklearn) как обучаемая модель.
1. Преобразование данных используется в составе конвейеров.
1. В процессе исследования бывает необходимо попробовать разные методы преобразования данных в комбинации с разными моделями обучения.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div> -->




<!-- ### Работа с нестандартными типами данных

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы работы с изображениями

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы векторизации текстов

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Работа с временными рядами

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Векторизация аудиоданных

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы работы с видеоданными

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div> -->