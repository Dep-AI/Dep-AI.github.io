---
section: ml
title: "Предварительный анализ и обработка данных"
---

### Сбор данных для обучения

#### Реляционная форма - объекты, атрибуты и признаки

![](/assets/images/ml_text/ml5-1.png "dataset"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Датасет - это набор данных, используемый для обучения моделей.
1. Объекты - это элементарные сущности, которые мы изучаем, объекты реального мира, измерения, наблюдения.
1. Каждый объект характеризуется набором атрибутов. В датасете у всех объектов одинаковый набор атрибутов, а значения - разные.
1. Атрибут или переменная - это свойство объектов в датасете, признак - это колонка данных, которая подается на вход модели машинного обучения.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Понятие чистых данных

![](/assets/images/ml_text/ml5-1.png "dataset"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Данные представлены в виде единой таблицы.
1. Строки таблицы представляют собой измерение, точку данных, объект предметной области.
1. Колонки таблицы представляют собой атрибуты объектов, признаки, переменные.
1. Каждая таблица, файл представляет собой данные об одном виде наблюдений или экспериментов.
1. Дополнительно: Все данные должны быть выражены в численном виде.
1. Дополнительно: В данных не должно быть отсутствующих (пропущенных) значений. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Оценка источников и объемов данных

{% capture notice %}
Выводы:
1. После постановки задачи машинного обучения первый этап моделирования - определение источников и объема данных, необходимых для эффективного обучения.
1. Объем данных определяется порядком сложности задачи: чем больше данных, тем более сложные модели можно на них строить.
1. В целом, чем больше данных можно собрать, тем лучше.
1. Данные в датасете должны быть репрезентативной выборкой генеральной совокупности.
1. Источники данных могут быть открытые и закрытые, платные, по подписке, внутренние и внешние.
1. Данные могут быть пакетные и потоковые, с ними надо работать по разному.
1. Данные можно собирать, генерировать и модифицировать.
1. Data augmentation - отдельная дисциплина.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Интеграция данных

{% capture notice %}
Выводы:
1. Интеграция данных - это процесс объединения данных из нескольких источников в единый датасет.
1. Объединение датасетов может происходить по горизонтали и по вертикали.
1. Вертикальное объединение датасетов - это по сути просто склеивание двух однотипных наборов данных в один.
1. Горизонтальное объединение датасетов происходит обязательно через аналог операции JOIN по ключу.
1. При объединении данных из разных источников необходимо особенно тщательно следить за обозначениями, соглашениями, датами.
1. Интеграция данных - самая трудная часть работы с машинным обучением.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Описательный анализ данных (EDA)

![](/assets/images/ml_text/ml5-3.png "eda"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Описательный анализ данных нужен для обнаружения проблем (артефактов) в данных и для выбора метода их устранения.
1. EDA также может дать информацию о структуре данных, шкалах измерения переменных, которые потом повлияют на методы обработки данных.
1. Также EDA может помочь при выборе признаков, выявлении зависимостей в данных.
1. EDA - это обзорный анализ данных именно при их подготовке к процессу машинного обучения.
1. При необходимости, можно провести более глубокий статистический анализ данных.
1. Анализ и обработку данных обычно производят параллельно, это не изолированные процессы.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Анализ репрезентативности датасета

![](/assets/images/ml_text/ml5-24.png "репрезентативность"){: .align-center style="width: 800px;"}

Не самым распространенным, но, несомненно, полезным этапом описательного анализа данных является анализ репрезентативности выборки. Конечно, не в каждой задаче исследователь имеет доступ к информации о генеральной совокупности объектов описываемой предметной области, однако, в случае наличия информации, даже простой подсчет, какой процент ГС описывает обучающий набор данных может дать представление о потенциальной обобщающей возможности модели машинного обучения.
Кроме того, на данном этапе может выполняться проверка адекватности распределения признаков теоретическим или эмпирическим экспертным знаниям о предметной области. 

Основные задачи:

- Анализ репрезентативности выборки;
- Оценка целесообразности моделирования.

{% capture notice %}
Выводы:
1. Для машинного обучения важно, чтобы датасет правильно отражал генеральную совокупность. Это дает модели обобщающую способность.
1. Анализ соотношения выборки и генеральной совокупность не всегда возможно провести, но когда можно - нужно это делать.
1. Самая простая метрика - процент ГС, который отражен в выборке.
1. Важно понимать, как были собраны данные, по какому принципу они отбирались. Лучше - случайно.
1. Может быть проведена проверка адекватности распределения признаков экспертным знаниям в области.
1. На этом этапе принимается общее решение о целесообразности моделирования.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Описание шкалы измерения атрибутов

![](/assets/images/ml_text/ml5-2.png "Типы шкал"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Шкалы атрибутов показывают, какие значения может принимать этот атрибут и как его можно интерпретировать и сравнивать.
1. Все атрибуты подразделяются на численные (непрерывные) и категориальные (дискретные). 
1. Категориальные переменные потом придется преобразовать в численные.
1. Номинальная шкала - это признак, для которого имеет смысл только равенство. Пример - метка класса.
1. Ординальная шкала - это когда наряду с равенством мы можем выстроить значения по порядку, который имеет смысл. Пример - класс обслуживания.
1. Ординальные признаки можно преобразовывать методом LabelEncoder. Номинальные - только OneHotEncoder.
1. Интервалая шкала - это когда еще имеет смысл говорить о разнице между двумя значениями. Пример - даты.
1. Абсолютная шкала - это когда еще можно говорить о том, во сколько раз одно значение больше другого. Пример - сумма денег.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Визуализация распределения атрибутов

![](/assets/images/ml_text/ml5-5.png "визуализация распределения признака"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-6.png "визуализация нестандартного признака"){: .align-center style="width: 800px;"}

Также на начальном этапе зачастую строят индивидуальное эмпирическое распределение каждого признака, что позволяет выдвинуть предварительную гипотезу о виде распределения соответствующей переменной в генеральной совокупности. Вид распределения может оказать влияние на выбор метода нормализации данных (по среднему, по разбросу или стандартизация), выявить проблему смещенных классов, особенно острую в распределении по целевой переменной, индицировать системные ошибки выборки. Анализ индивидуального распределения может также помочь выявить аномальные выбросы, ошибки измерений, или опечатки в данных.

{% capture notice %}
Выводы:
1. Оценка эмпирического распределения атрибутов дает ценную информацию, которая определяет дальнейшую работу с этим атрибутом.
1. Можно использовать статистические методы, но обычно достаточно визуально оценить распределение по графику.
1. Категориальные переменные обычно изображают на графике, или списком в порядке убывания количества объектов.
1. Для численных переменных обычно строят график плотности распределения, хотя иногда - тоже гистограмму.
1. Для категориальных переменных оценивают долю каждого класса в датасете, сбалансированность распределения, моду.
1. Для численных переменных оценивают минимальное и максимальное значение, меры центрального элемента и разброса.
1. Для численных переменные еще оценивают вид распределения - равномерное, нормальное, логнормальное, экспоненциальное и т.д.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Проблема несбалансированных классов

![](/assets/images/ml_text/ml5-7.png "это еще не дисбаланс"){: .align-center style="width: 800px;"}

![Дисбаланс классов](https://miro.medium.com/max/425/1*GkhP_fliGHcijoiyp0I4Wg.png "Дисбаланс классов"){: .align-center style="width: 800px;"}
Источник: [analyticsvidhya.com](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2020%2F07%2F10-techniques-to-deal-with-class-imbalance-in-machine-learning%2F&psig=AOvVaw3luN7CCjqQGkXMf93CNqN3&ust=1670430010475000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCLCIp-2y5fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

![Множественный дисбаланс](https://www.researchgate.net/publication/344485627/figure/fig1/AS:943477185585154@1601953898776/a-Class-imbalanced-data-distribution-of-EGTEA-dataset-b-Class-wise-classification.png "Множественный дисбаланс"){: .align-center style="width: 800px;"}
Источник: [ResearchGate](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.researchgate.net%2Ffigure%2Fa-Class-imbalanced-data-distribution-of-EGTEA-dataset-b-Class-wise-classification_fig1_344485627&psig=AOvVaw3luN7CCjqQGkXMf93CNqN3&ust=1670430010475000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCLCIp-2y5fsCFQAAAAAdAAAAABAW).
{: style="text-align: center; font-size:0.7em;"}

![К чему приводит дисбаланс](https://i.stack.imgur.com/boL50.png "К чему приводит дисбаланс"){: .align-center style="width: 800px;"}
Источник: [DSSE](https://www.google.com/url?sa=i&url=https%3A%2F%2Fdatascience.stackexchange.com%2Fquestions%2F72958%2Fwhat-does-the-classification-report-interpret-class-1-indicates-abnormal-data&psig=AOvVaw0r0neS3g1ImN1IgVY085er&ust=1670430330798000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCICs9oW05fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Сбалансированность классов - это равномерность распределения целевой переменной в задачах классификации.
1. Сильно несбалансированные классы плохо сказываются на эффективности обучения, так как модель подстраивается под мажоритарный класс.
1. Самый простой способ устранения - удаление случайной части объектов мажоритарного класса, но это приводит к кратному уменьшения датасета.
1. Можно попробовать добавить объектов миноритарных классов, но это не всегда возможно.
1. С несбалансированными классами неплохо справляются иерархические классификаторы.
1. Можно попробовать модифицировать алгоритм обучения, придав классам веса.
1. Если ничего не помогает, возможно следует пересмотреть постановку задачи, разбить мажоритарный класс, переформулировать проблему.
1. Нужно проводить анализ ошибок
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Оценка влияния атрибутов на целевую переменную

В случае задачи обучения с учителем весьма полезно оценить форму совместного распределения целевой переменной с каждым признаком в отдельности. Это позволяет сделать первичное эмпирическое предположение о влиянии каждого фактора на результирующую переменную в случае обнаружения распределения, сильно отличающегося от равномерного.

Основные задачи:

- Выдвижение гипотез об относительной важности факторов;
- Обоснование введения суррогатных факторов.

![](/assets/images/ml_text/ml5-8.png "непрерывная-дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-9.png "дискретная дискретная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-10.png "дискретная бинарная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-11.png "табличная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-16.png "ящик с усами"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-15.png "дискретная-непрерывная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-12.png "сложная"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-13.png "таблица с гистограммой"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-14.png "фасетная"){: .align-center style="width: 800px;"}

По аналогии с предыдущим этапом, довольно полезную информацию дает визуальная оценка совместного распределения каждой пары признаков и целевой переменной. Конечно, обратной стороной является квадратичная зависимости количества пар признаков, что делает практически неприменимым исчерпывающий анализ по всем возможным парам при более-менее большом количестве атрибутов в наборе данных. Поэтому исследуются чаще всего только те пары, которые могут представлять интерес, как по соображениям предметной области, так и по результатам анализа индивидуального влияния признаков.

Основные задачи - те же, что и на предыдущем этапе.

![](/assets/images/ml_text/ml5-17.png "три признака"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-18.png "три признака ирисы"){: .align-center style="width: 800px;"}

![feature importance](https://miro.medium.com/max/1400/0*vKRFyD57QMHC1WAp "feature importance"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-feature-importance-and-how-to-implement-it-in-python-ff0287b20285&psig=AOvVaw25OTcH_aslwWvq9xGRX-9S&ust=1670433370537000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCJCIuK-_5fsCFQAAAAAdAAAAABAc).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Очень полезно оценить влияние каждого атрибута на целевую переменную.
1. Для этого обычно строят совместное распределение атрибута и целевой переменной. Какой тип визуализации использовать - зависит от шкал.
1. Если обе переменные численные - строят диаграмму рассеяния.
1. Если обе переменные категориальные - можно строить гистограммы с несколькими столбцами или таблицы (часто раскрашивают).
1. Если шкалы разные, то обычно строят график с несколькими линиями.
1. При необходимости можно строить графики совместного распределения двух атрибутов и целевой переменной. Но они уже более сложные.
1. Этот этап может дать информацию о том, какие атрибуты важны, а какие можно и удалить из модели.
1. Можно строить простые модели, чтобы анализировать feature importance.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Построение корреляционной матрицы

Построение корреляционной матрицы дает очень наглядное и полное представление о влиянии каждого атрибута как на целевую переменную, так и на другие факторы, поэтому многие исследователи используют ее как завершающий этап описательного анализа данных. Однако, недостатком корреляционной матрицы можно является учет только факторов, выраженных в числовых шкалах.
Корреляционная матрица прекрасно выявляет мультиколлинеарность факторов и является первой диагностикой для исключения факторов из дальнейшего моделирования.

Основные задачи:

- Выдвижение гипотезы об относительной важности факторов
- Обнаружение мультиколлинеарности факторов

![](/assets/images/ml_text/ml5-20.png "маленькая коррелограмма"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-21.png "большая коррелограмма"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-22.png "матрица совместных распределений"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-23.png "большая матрица"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Коррелограмма - это популярный инструмент визуализации, он дает много информации, а построить его просто.
1. Корреляционная матрица дает представление о степени влияния каждого фактор на каждый и на целевую переменную.
1. Корреляционная матрица по своей природе симметрична относительно главной диагонали, в которой стоят единицы.
1. Коррелограмма может также дать информацию о мультиколлинеарности атрибутов.
1. Стоит обращать внимание как на положительные, так и на отрицательные корреляции.
1. Коррелограмма показывает только линейные зависимости.
1. По необходимости еще строится матрица совместных распределений.
1. Можно проводить и до и после преобразования данных, но если атрибутов слишком много, матрица получится перегруженной.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Обнаружение аномальных объектов в датасете

Задача обнаружения аномалий сама по себе представляет собой область машинного обучения. Методы и инструменты данной сферы иногда применяются для предварительного анализа данных в целях идентификации выбросов, выбивающихся из общего ряда наблюдений значений конкретного признака. Несмотря на то, что задача обнаружения выбросов, несомненно, полезна для анализа данных, использование такого сложного и затратного инструмента редко может быть оправдано. 

Основные задачи:

- Обнаружение аномальных объектов, потенциально ошибок в данных, потенциально нерелевантных членов выборки.

![](/assets/images/ml_text/ml5-25.png "без аномалий"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-26.png "может быть аномалия"){: .align-center style="width: 800px;"}

![](/assets/images/ml_text/ml5-27.png "выброс"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Аномальные объекты - это те, которые по своим значениям сильно отличаются от большинства.
1. Часто аномалии означают ошибки в данных, опечатки, значения вне разумного диапазона для атрибута.
1. Выбросы (единичные значения атрибута, далеко отстоящие от всех соседей) - почти всегда свидетельствуют об ошибках в данных.
1. Аномалии можно увидеть на графиках, но если атрибутов слишком много, не все аномалии могут быть выявлены.
1. Обнаружение аномалий - сама по себе задача машинного обучения, которая может выявить аномалии не только по одному измерению, а по всем.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Очистка и преобразование данных

#### Удаление лишних признаков (feature selection)

{% capture notice %}
Выводы:
1. Выбор признаков - важная часть преобразования данных, ведь чем точнее мы определим необходимую для моделирования информацию, тем эффективнее будет проходить обучение.
1. Не стоит оставлять в датасете ненужные признаки - это повышает вариативность моделей и может приводить к переобучению.
1. Для выбора признаком часто используют выводы из EDA, обучение простых моделей (DT, RF) или здравый смысл.
1. Выбор признаков можно автоматизировать - применять алгоритм очередного удаления или добавления признаков.
1. Выбор признаков менее актуален, если обучаются нейронные сети, они способны сами отбирать признаки.
1. Регуляризация обычно работает лучше отбора признаков.
1. Совсем лишние данные все равно надо убирать.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Удаление непоказательных объектов

{% capture notice %}
Выводы:
1. Аномальные объекты часто удаляются из датасета, чтобы не искажать результаты обучения.
1. Если можно исправить выброс, можно попытаться это сделать.
1. Следует анализировать, показателен ли объект для предметной области. Если нет - можно смело удалять.
1. После удаления множества объектов может понадобиться повторить анализ сбалансированности классов.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Заполнение отсутствующих значений

![](/assets/images/ml_text/ml5-4.png "отсутствующие значения"){: .align-center style="width: 800px;"}

![как выглядят пропуски](https://vitalflux.com/wp-content/uploads/2020/07/Screenshot-2020-07-23-at-12.07.27-PM.png "как выглядят пропуски"){: .align-center style="width: 800px;"}
Источник: [Data Analytics](https://www.google.com/url?sa=i&url=https%3A%2F%2Fvitalflux.com%2Fpandas-fillna-method-imputing-missing-values%2F&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![заполнение средним](https://miro.medium.com/max/1400/1*A7RpRo96poWAnyFcPoR_Jg.png "заполнение средним"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fa-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98&psig=AOvVaw2J7fJ_IF6YN1FJCb8xHV4j&ust=1670430529116000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNjIsuS05fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. При выборе стратегии борьбы с пропусками в данных следует проанализировать, сколько пропусков и где они расположены.
1. Если по одному из признаков большинство значений пропущено, можно задуматься об удалении этого признака из датасета.
1. Если наоборот, по одному объекту много неизвестных атрибутов, можно удалить объект.
1. Следует следить, чтобы от датасета что-то осталось после массового удаления.
1. Самый простой способ заполнить пропуски - заполнить их средним значением, но это сильно искажает форму распределения признака.
1. Зачастую можно считать групповое среднее, более "индивидуальную" оценку атрибута данного объекта, с учетом значений других атрибутов.
1. Иногда применяют заполнение случайным значением. Его лучше всего генерировать из распределения данного признака.
1. Самое грамотное решение - введение нового признака или заполнение специальным значением (по категориальным признакам).
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Создание суррогатных признаков (feature engineering)

{% capture notice %}
Выводы:
1. Инжиниринг признаков - сложная и творческа работа, но зачастую именно в ней кроется секрет существенного повышения эффективности моделей.
1. Суррогатные признаки - это как вычислимые поля в базах данных, они создаются на основе уже присутствующих в датасете атрибутов.
1. При создании новых признаков нужно руководствоваться здравым смыслом и знанием предметной области.
1. Зачастую новые признаки - это часть существующих, которые потом удаляются.
1. Например, из фамилии можно получить список родственников, из даты рождения - возраст и так далее.
1. После введения суррогатных признаков следует повторить EDA, хотя бы частично.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Преобразование численных атрибутов в категориальные

Довольно часто исследователи проводят анализ количества объектов в обучающем наборе данных, удовлетворяющим каким-либо условиям, вытекающим из описания предметной области, например, попадающим в определенный промежуток значений, обладающих уникальным набором характеристик, соответствующих конкретным значениям количественной характеристики (чаще всего, 0 или 1). Такой анализ позволяет выявить и исключить из выборки нерелевантные объекты.

Основные задачи:

- Выявление неочевидных кластеров объектов предметной области;
- Выявление непоказательных объектов в наборе данных.

![](/assets/images/ml_text/ml5-19.png "группировка в титанике"){: .align-center style="width: 800px;"}

{% capture notice %}
Выводы:
1. Иногда бывает целесообразно сгруппировать объекты датасета по значению какого-то признака и заменить его названием группы.
1. В таком случае, мы удаляет часть информации из модели, но это может быть лишняя вариативность.
1. Показательный пример - группы населения по возрасту.
1. Такое нужно делать, только если есть уверенность, что объекты внутри группы одинаково относятся к целевой переменной.
1. Границы групп выбирают вручную, от этого многое зависит.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Преобразование категориальных атрибутов

![Labelencoder](https://miro.medium.com/max/386/1*Yp6r7m82IoSnnZDPpDpYNw.png "Labelencoder"){: .align-center style="width: 800px;"}
Источник: [Medium](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40chexki_%2Fusing-label-encoder-on-unbalanced-categorical-data-in-machine-learning-using-python-435f521323b1&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAJ).
{: style="text-align: center; font-size:0.7em;"}

![labelencoder done right](https://miro.medium.com/max/588/1*KdCvKnI9ATVPiozmuRindA.png "labelencoder done right"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fcategorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd&psig=AOvVaw2bNidrApYqol6ih-aak0p3&ust=1670432731162000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCLyv685fsCFQAAAAAdAAAAABAO).
{: style="text-align: center; font-size:0.7em;"}

![one hot encoder](https://codecamp.ru/content/images/2021/09/oneHot1.png "one hot encoder"){: .align-center style="width: 800px;"}
Источник: [Кодкамп](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.codecamp.ru%2Fblog%2Fone-hot-encoding-in-python%2F&psig=AOvVaw1SjP5tmHbyYdNMFaBhJ2OR&ust=1670432674003000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCODul-O85fsCFQAAAAAdAAAAABAN).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Категориальные признаки часто выражаются строковыми данными и не подходят для использования в машинном обучении, их преобразуют в численные.
1. Самый простой кодировщик - LabelEncoder - просто нумерует все значения категориального признака.
1. Он вводит порядок в значения категорий, которого раньше не было, поэтому можно исказить результаты обучения.
1. Исключение - бинарные признаки, их можно кодировать как 0 и 1.
1. Более продвинутый кодировщик - OneHotEncoder - преобразует один признак во множество.
1. Новые признаки соответствуют значениям исходного и кодируются бинарно. Этот способ более универсален и рекомендуется применять по умолчанию.
1. В принципе, один из получившихся признаков можно удалить, но обычно никто не заморачивается.
1. Да, из одного признака может получиться тысяча. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Нормализация и решкалирование признаков

Минимаксная нормализация - это изменение входных данных по следующей формуле:

{% capture block %}
$$ x' = \frac{x - x_{min}}{x_{max} - x_{min}} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

После преобразования все значения будут лежать в диапазоне $x \in [0; 1]$. 

Z-оценки или стандартизация производится по формуле:

{% capture block %}
$$ x' = \frac{x - M[x]}{\sigma_x} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

В таком случае данный признак приводится к стандартному распределению, то есть такому, у которого среднее 0, а дисперсия - 1. 

![scaler](https://miro.medium.com/max/1200/1*UPLv3kNw9JTtNabr70dQDQ.png "scaler"){: .align-center style="width: 800px;"}
Источник: [TDS](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-and-why-to-standardize-your-data-996926c2c832&psig=AOvVaw0vaCY5M7N0Ieg06Xjz_Et6&ust=1670432861032000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCNCTtLy95fsCFQAAAAAdAAAAABAE).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Нормализация признаков нужна для ускорения обучения и сходимости градиентного спуска.
1. Во многих реализациях моделей нормализация уже встроена и применяется по умолчанию.
1. Основная идея нормализации - это сделать так, чтобы все признаки измерялись по одной шкале, то есть лежали в одних пределах.
1. Обычно для этого выбирают шкалу от 0 до 1. Тогда каждое значение нужно разделить на максимальное.
1. Еще применяют стандартизацию - приведение к стандартному распределению.
1. Нормализация - это параметрическое преобразование, нужно запоминать, на что делили или что отнимали.
1. О отдельных случая применяются более крутые алгоритмы решкалирования с автоматическим устранением выбросов.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Краткий сценарий анализа данных

{% capture notice %}
Выводы:
1. Интеграция и очистка данных.
1. Подробное описание каждого признака, шкалы, вида распределения.
1. Исследование отсутствующих значений, заполнение или удаление.
1. Описание вида распределения каждого значимого признака и целевой переменной.
1. Преобразования категориальных признаков в численные.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Подробный сценарий анализа данных

{% capture notice %}
Выводы:
1. Оценка источников и объемов данных
1. Анализ репрезентативности и однородности разделения выборки.
1. Гипотезы о виде распределения каждого признака и выявление аномалий, визуализация.
1. Выявление и исправление несбалансированности классов.
1. Корреляционная матрица, выявление мультиколлинеарности, важность признаков.
1. Отбор признаков, инжиниринг признаков, возможная группировка значений.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Воспроизводимость преобразования данных

{% capture notice %}
Выводы:
1. Алгоритм, по которому выполнялось преобразование обучающей выборки необходимо сохранить, так как именно такое же преобразование нужно будет проделать над тестовой.
1. Да, преобразование данных обычно проводят уже после разделения выборки на обучающую и тестовую.
1. При параметрических преобразованиях все значения параметров подбираются именно по обучающей выборке.
1. Это может означать, что в тестовой после нормализации признак может быть больше 1 (или меньше 0), это нормально, так и надо.
1. Если используется преобразование данных, то модель будет работать на преобразованных данных, а из реального мира будут приходить исходные. 
1. Обычно преобразование данных оформляется как функция или (в sklearn) как обучаемая модель.
1. Преобразование данных используется в составе конвейеров.
1. В процессе исследования бывает необходимо попробовать разные методы преобразования данных в комбинации с разными моделями обучения.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

<!-- ### Работа с нестандартными типами данных

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы работы с изображениями

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы векторизации текстов

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Работа с временными рядами

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Векторизация аудиоданных

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

#### Методы работы с видеоданными

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div> -->