---
section: ml
title: "Задача регрессии"
---


### Линейная регрессия с одной переменной


#### Представление модели

Напомним, что в задачах регрессии мы принимаем входные переменные и пытаемся подогнать выход на непрерывную ожидаемую функцию результата. Линейная регрессия с одной переменной также известна как «парная линейная регрессия».

Одномерная линейная регрессия используется, когда вы хотите предсказать одно выходное значение _y_, зависящее от одного входного значения _x_. Мы проводим обучение с учителем, это означает, что мы уже имеем представление о том, какие значения выходной переменной соответствуют некоторым значениям входной переменной.


#### Функция гипотезы

Наша прогностическая функция (функция гипотезы, модель) имеет общий вид:



$$ \hat{y} = h_b (x) = b_0 + b_1 x $$


Обратите внимание, что это похоже на уравнение прямой. в данном случае, мы пытаемся подобрать функцию _h(x)_ таким образом, чтобы отобразить данные нам значения _x_ в данные значения _y_.

Допустим, мы имеем следующий обучающий набор данных:


<table>
  <tr>
   <td>входная переменная x
   </td>
   <td>выходная переменная y
   </td>
  </tr>
  <tr>
   <td>0
   </td>
   <td>4
   </td>
  </tr>
  <tr>
   <td>1
   </td>
   <td>7
   </td>
  </tr>
  <tr>
   <td>2
   </td>
   <td>7
   </td>
  </tr>
  <tr>
   <td>3
   </td>
   <td>8
   </td>
  </tr>
</table>


Мы можем составить случайную гипотезу с параметрами $ b_0 = 2, b_1 = 2 $. Тогда для входного значения $ x=1, y=4 $, что на 3 меньше данного. Задача регрессии состоит в нахождении таких параметров функции гипотезы, чтобы она отображала входные значения в выходные как можно более точно, или, другими словами, описывала линию, наиболее точно ложащуюся в данные точки на плоскости (x, y). 


#### Функция ошибки

Мы можем измерить точность нашей функции гипотезы, используя функцию ошибки. Для этого требуется средняя (фактически чуть усложненная версия среднего арифметического) всех результатов вычисления гипотезы с входами x по сравнению с фактическим выходом y.



$$ 
J(b_0, b_1) 
= \frac{1}{2m} \sum_{i=1}^{m} (\hat{y_i} - y_i)^2 
= \frac{1}{2m} \sum_{i=1}^{m} (h_b(x_i) - y_i)^2 
$$


По сути своей, это половина среднего квадрата разницы между прогнозируемым и фактическим значением выходной переменной.

Эту функцию называют «функцией квадрата ошибки» или «среднеквадратичной ошибкой» (mean squared error, MSE). Среднее значение уменьшено вдвое для удобства вычисления градиентного спуска, так как производная квадратичной функции будет отменять множитель 1/2.

Теперь мы можем конкретно измерить точность нашей предсказывающей функции по сравнению с правильными результатами, которые мы имеем, чтобы мы могли предсказать новые результаты, которых у нас нет.

Если мы попытаемся представить это наглядно, наш набор данных обучения будет разбросан по плоскости x-y. Мы пытаемся подобрать прямую линию, которая проходит через этот разбросанный набор данных. Наша цель - получить наилучшую возможную линию. Лучшая линия будет такой, чтобы средние квадраты вертикальных расстояний рассеянных точек от линии были наименьшими. В лучшем случае линия должна проходить через все точки нашего набора данных обучения. В таком случае значение J будет равно 0.


#### Метод градиентного спуска

Таким образом, у нас есть наша функция гипотезы, и у нас есть способ оценить, насколько хорошо конкретная гипотеза вписывается в данные. Теперь нам нужно подобрать параметры в функции гипотезы. Вот где приходит на помощь метод градиентного спуска.

Представьте себе, что мы нарисуем нашу функцию гипотезы на основе ее параметров b<sub>0</sub> и b<sub>1</sub> (фактически мы представляем график функции стоимости как функцию оценок параметров). 

Отложим b<sub>0</sub> на оси x и b<sub>1</sub> на оси y, с функцией стоимости на вертикальной оси z. Точки на нашем графике будут результатом функции стоимости, используя нашу гипотезу с этими конкретными параметрами.

Мы будем знать, что нам удалось подобрать оптимальные параметры, когда наша функция стоимости находится в самом низу на нашем графике, то есть когда ее значение является минимальным.

То, как мы это делаем - это используя производную (касательную линию к функции) нашей функции стоимости. Наклон касательной является производной в этой точке, и это даст нам направление движения в сторону самого крутого уменьшения значения функции. Мы делаем шаги вниз по функции стоимости в направлении с самым крутым спусками, а размер каждого шага определяется параметром α, который называется скоростью обучения.

Алгоритм градиентного спуска:

повторяйте до сходимости: 


$$ b_j := b_j - \alpha \frac{\partial}{\partial b_j} J(b_0, b_1) $$


где j=0,1 - представляет собой индекс номера признака.


### Линейная регрессия с несколькими переменными

Линейная регрессия с несколькими переменными также известна как «множественная линейная регрессия». Введем обозначения для уравнений, где мы можем иметь любое количество входных переменных:

$ x^{(i)} $- вектор-столбец всех значений признаков i-го обучающего примера;


$ x_j^{(i)} $ - значение j-го признака i-го обучающего примера;

_m_ - количество примеров в обучающей выборке;

_n_ - количество признаков;

_X_ - матрица признаков;

_b_ - вектор параметров регрессии.

Заметим, что в будущем для удобства примем, что $ x_0^{(i)} = 1 $ для всех i. Другими словами, мы для удобства введем некий суррогатный признак, для всех наблюдений равный единице. это сильно упростит математические выкладки, особенно в матричной форме. 

Теперь определим множественную форму функции гипотезы следующим образом, используя несколько признаков:


$$ h_b(x) = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n $$


Используя определение матричного умножения, наша многопараметрическая функция гипотезы может быть кратко представлена в виде: _h(x) = B X_.


#### Функция ошибки

Для множественной регрессии функция ошибки от вектора параметров b выглядит следующим образом:


$$ J(b) = \frac{1}{2m} \sum_{i=1}^{m} (h_b(x^{(i)} - y^{(i)})^2 $$


Или в матричной форме:


$$ J(b) = \frac{1}{2m} (X b - \vec{y})^T (X b - \vec{y}) $$


Метод градиентного спуска для множественной регрессии определяется следующими уравнениями:

повторять до сходимости:


$$ b_0 := b_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_b(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)} $$


$$ b_1 := b_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_b(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} $$


$$ b_2 := b_2 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_b(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} $$


$$ ... $$


Или в матричной форме:



$$ b := b - \frac{\alpha}{m} X^T (X b - \vec{y}) $$



#### Нормализация признаков

Мы можем ускорить сходимость метода градиентного спуска, получив каждое из наших входных значений примерно в том же диапазоне. Это связано с тем, что b будет быстро сходиться на малых диапазонах и медленно на больших диапазонах, и поэтому будет колебаться неэффективно до оптимального, если переменные очень неравномерны.

Способ предотвратить это - изменить диапазоны наших входных переменных, чтобы они были примерно одинаковыми. В идеале 

-1 ≤ x ≤ 1 или же -0,5 ≤ x ≤ 0,5.

Это не точные требования; мы только пытаемся ускорить процесс. Цель состоит в том, чтобы получить все входные переменные в примерно один из этих диапазонов, дать или взять несколько.

Два метода для этого - масштабирование признаков и нормализация по среднему. Масштабирование признаков заключается в делении входных значений на размах выборки (то есть максимальное значение минус минимальное значение) входной переменной, в результате чего новый диапазон составляет всего 1. Нормализация по среднему включает в себя вычитание среднего значения входной переменной из значений для этой входной переменной, в результате чего новое среднее значение для этой переменной равно нулю. Чтобы реализовать оба этих метода, отрегулируйте свои входные значения, как показано в этой формуле:


$$ x_i = \frac{x_i - \mu_i}{s_i} $$


Где 

$ \mu_i $- среднее значение признака i, а 

$ s_i $ - стандартное отклонение этого признака.


#### Советы по методу градиентного спуска

1. Отладка градиентного спуска. 
Сделайте график с количеством итераций по оси x. Теперь построим функцию стоимости J(b) по числу итераций градиентного спуска. Если J(b) когда-либо возрастает, то, вероятно, вам необходимо уменьшить α.

2. Автоматический тест сходимости. 
Объявите сходимость, если J(b) уменьшится меньше чем на E на одной итерации, где E - некоторое небольшое значение, такое как 10<sup>-3</sup>. Однако на практике трудно выбрать это пороговое значение.
Было доказано, что если скорость обучения α достаточно мала, то J(b) будет уменьшаться на каждой итерации. 

3. Суррогатные признаки
Мы можем улучшить наши функции и форму нашей функции гипотез несколькими способами. Мы можем объединить несколько признаков в один. Например, мы можем объединить x1 и x2 в новый признак x3, взяв x1⋅x2.


#### Полиномиальная регрессия

Наша функция гипотезы не обязательно должна быть линейной (прямой), если это не соответствует данным.

Мы можем изменить поведение или кривую нашей функции гипотезы, сделав ее квадратичной, кубической или квадратной корневой функцией (или любой другой формой).

Например, если наша функция гипотезы 
$ \hat{y} = h_b (x) = b_0 + b_1 x $, 
то мы можем добавить еще один признак, основанный на x1, получив квадратичную функцию 
$ \hat{y} = h_b (x) = b_0 + b_1 x + b_2 x^2 $
или кубическую функцию 
$ \hat{y} = h_b (x) = b_0 + b_1 x + b_2 x^2 + b_3 x^3 $. 
В кубической функции мы по сути ввели два новых признака: 
$ x_2 = x^2, x_3 = x^3 $. 
Точно таким же образом, мы можем создать, например, такую функцию: 
$ \hat{y} = h_b (x) = b_0 + b_1 x + b_2 \sqrt{x} $.

Одна важная вещь, о которой следует помнить, заключается в том, что если вы выбираете свои функции таким образом, масштабирование признаков становится очень важным. Например, если x имеет диапазон 1 - 1000, тогда диапазон x<sup>2</sup> становится 1 - 1000000, а диапазон x<sup>3</sup> становится 1 - 1000000000.


#### Нормальное уравнение

«Нормальное уравнение» - это метод нахождения оптимальных параметров регрессии без итераций:


$$ b = (X^T X)^{-1} X^T y $$


Нет необходимости выполнять масштабирование признаков, если мы решаем регрессию с помощью нормального уравнения.

Метод решения через нормальное уравнение имеет ряд преимуществ по сравнению с методом градиентного спуска:



1. Нет необходимости в нормализации признаков;
2. Не нужно выбирать скорость обучения;
3. Не требует вычисления частных производных функции ошибки;

Однако, у него есть и недостатки:



1. Имеет асимптотику O(n<sup>3</sup>) по сравнению с O(n<sup>2</sup>) у градиентного спуска. Поэтому довольно медленно работает при больших n.
2. Требует вычисления обратной матрицы. В некоторых случаях матрица $X^T X$ может быть вырожденной, что затруднит использование нормального уравнения.
