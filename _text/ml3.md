---
section: ml
title: "Методы обучения с учителем"
---

### Линейные модели

В предыдущих главах мы изучили применение метода линейной регрессии для предсказания непрерывной целевой переменной и логистической регрессии для решения задач классификации. Можно заметить, что по сути это одна и та же модель, но немного по-другому использующаяся при решении разных задач. В основе обеих этих моделей лежит использование линейной функции. Мы подбираем параметры этой линейной функции таким образом, чтобы она удовлетворяла определенным оптимизационным критериям. И вот сами эти критерии различаются для задач регрессии и классификации.

И в той и в другой задаче мы имеем обучающую выборку, которую можно представить как некоторое множество точек в многомерном пространстве. Причем, в задачах регрессии мы обычно воспринимаем целевую переменную как еще одно измерение. Поэтому на двумерных графиках мы часто изображаем регрессию от одной переменной (то есть с одним атрибутом _x_, одной колонкой в наборе данных) - вместе с целевой переменной получится как раз два измерения. А вот в задачах классификации значения целевой переменной, то есть класс объекта, обычно не воспринимается как отдельное измерение, а как некоторая встроенная характеристика точки. Поэтому мы можем легко изобразить на двумерном графике задачу классификации на двух признаках ($x_1$ и $x_2$) - ведь метку класса мы изображаем как цвет или форму точки.

Но помните, как в начале прошлой главы мы пытались изобразить классификацию также как регрессию - откладывая целевую переменную по одной из осей? На самом деле размерность задачи определяется только количеством признаков, независимо от того, какую задачу мы решаем. Если у нас один признак - то мы имеем распределение точек на плоскости. Если два - в пространстве, а если больше - в гиперпространстве. Представить это становится все труднее, поэтому мы учимся на примерах маленькой размерности. Важно понять, как устроены модели и методы, а математический аппарат способен обобщить наши подходы на произвольное количество измерений.

А линейные модели устроены таким образом, что они описывают некоторое линейное подпространство с размерностью, на единицу меньшей, чем размерность всей задачи в целом. Если в модели один признак - то линейная функция будет представлять собой прямую в той плоскости, в которой лежат точки такой двумерной задачи. Если два признака - то плоскость и пространстве трехмерной задачи (как на рисунке ниже). А если признаков больше - то гиперплоскость, вложенную в гиперпространство.

![Регрессия и классификация](https://www.researchgate.net/profile/Christian-Bauckhage/publication/330760775/figure/fig3/AS:721129559822338@1548942093486/binary-linear-classification-in-3D.png "Регрессия и классификация"){: .align-center style="width: 50%;"}
Источник: [ResearchGate](https://www.researchgate.net/publication/330760775_Lecture_Notes_on_Machine_Learning_Binary_Linear_Classifiers/figures?lo=1&utm_source=google&utm_medium=organic).
{: style="text-align: center; font-size:0.7em;"}

В любом случае мы находим линейную комбинацию координат этих точек с вектором весов, то есть по сути, некоторую гиперплоскость в этом многомерном пространстве. Однако то, какая именно линейная комбинация, будет для нас оптимальной, то есть какие коэффициенты в линейной функции мы хотим получить, зависит от задачи. Если мы решаем задачу регрессии, то мы строим линию таким образом, чтобы она лучше всего легла в точки. Причем здесь имеется в виду распределение точек вместе с целевой переменной. Как мы уже изучали, для этого нам нужна функция ошибки. То есть мы сравниваем значение функции гипотезы (модели) с истинными значениями целевой переменной и подбираем параметры модели таким образом, чтобы эти отклонения были как можно меньше.

![Линейная регрессия](/assets/images/ml_text/ml3-12.png "Линейная регрессия"){: .align-center style="width: 50%;"}

Немного другая ситуация с классификацией. Линейная модель также формирует гиперплоскость, но на этот раз мы не рассматриваем целевую переменную как еще одно измерение. То есть граница принятия решения должна быть на одно измерение меньше, чем размерность самой задачи. И мы как раз это и видим - для классификации нам не очень важно само значение функции гипотезы, нам важна область точек, в которых она равна 0. Эта область как раз и формирует границу принятия решения.

![Линейная классификация](/assets/images/ml_text/ml3-11.png "Линейная классификация"){: .align-center style="width: 50%;"}

Логистическая функция здесь нужна только для того, чтобы более надежно реализовать оптимизацию параметров методом градиентного спуска. Для точек, которые лежат по одну сторону границы мы хотим, чтобы линейная функция принимала положительные значения, а, соответственно, логистическая модель - значения, близкие к единице. И мы хотим, чтобы граница принятия решения располагалась таким образом, чтобы по эту сторону от нее располагались те точки, которые в обучающей выборке отнесены к положительному классу.

Другими словами, линейная и логистическая регрессии - суть одна и та же модель, использующаяся в разных условиях и с разными задачами. Поэтому, кстати, логистическая регрессия является линейной моделью. Вообще, стоит говорить об одном классе моделей - линейных моделей - которые применяются как к задачам регрессии, так и к задачам классификации.

Это справедливо и по отношению к другим классам моделей, которые мы будем рассматривать в данной главе. Очень редкие модели можно применять только к задачам классификации или к задачам регрессии. Мы вообще изучим только один такой пример - наивный байесовский классификатор, который по своей сути нельзя применить к регрессионным задачам. А в большинстве совем одни и те же модели могут использоваться для решения как задач предсказания дискретного, так и непрерывного значения. 

Поэтому мы не говорим отдельно об алгоритмах классификации или моделях регрессии. Есть модели обучения с учителем. Именно поэтому, кстати, в документации к библиотеке _sklearn_ вы не найдете отдельных частей, посвященных регрессии и классификации. Есть глава, посвященная линейным моделям, методу опорных векторов и так далее. И мы дальше будем изучать модели исходя именно из этой логики. 

Так как мы теперь говорим об общем классе линейных моделей, выделим главные их черты, которые отличают в практическом применении их от других видов моделей и алгоритмов машинного обучения. В первую очередь надо сказать, что линейные модели - это самый простой класс моделей. Это свидетельствует не только о примитивности (низкой вариативности) самого класса линейных функций. Главное - что линейные модели достаточно быстро обучаются. Существуют алгоритмы. которые требуют гораздо больших вычислительных мощностей для своей работы. Это несомненное преимущество линейных моделей.

Но из их простоты следует и главный недостаток. Линейные модели могут описать только линейные зависимости в данных. Для задач регрессии это означает, что линейные модели хорошо описывают данные, в которых присутствует линейная корреляция между признаками и целевой переменной. Если же в данных нет корреляции, либо зависимость имеет нелинейный характер, то линейные модели в принципе не способны хорошо (то есть с низкой ошибкой) описать это набор данных. Для задач классификации аналогичное свойство - линейная разделимость. Если граница между разными классами носит явно нелинейный характер, то логистическая регрессия просто не справится с разделением данных по классам.

{% capture notice %}
Надо помнить, что линейные зависимости в данных не означает, что данные точно описываются линейной функцией. Бывают ситуации, когда данные может быть формально и не являются линейно неразделимыми (в случае с классификацией) или не лежат на одной прямой (для регрессии), но явно демонстрируют линейную зависимость. Ведь мы строим модель, чтобы она научилась выявлять общие зависимости в данных, но в них часто есть и случайные колебания, которые не должны влиять на результат моделирования.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

![Линейная зависимость, но не разделимость](/assets/images/ml_text/ml3-49.png "Линейная классификация"){: .align-center style="width: 50%;"}

Сравните, например, такую картинку со следующей. На обоих изображены линейно неразделимые наборы данных. Но мы совершенно правильно интуитивно понимаем, что на верхнем графике изображены данные, которые очень хорошо будут описаны линейной моделью. И неважно, что несколько точек мы классифицируем неправильно. Даже наоборот, было бы хуже пытаться построить модель, которая бы учитывала эти аномальные точки и строила бы очень сложную границу принятия решений. 

![Нелинейная зависимость, но не разделимость](/assets/images/ml_text/ml3-50.png "Линейная классификация"){: .align-center style="width: 50%;"}

Еще одно достоинство, общее для линейных моделей - их интерпретируемость. Параметрами линейных моделей являются коэффициенты при признаках (плюс еще свободный коэффициент). И в любой реальной задаче признаки имеют какую-то интерпретацию, физический смысл. Поэтому их коэффициенты тоже можно объяснить в терминах предметной области. Коэффициент каждого признака показывает, на сколько изменится значение целевой переменной при изменении данного признака на единицу. То есть коэффициент косвенно показывает силу связи признака и целевой переменной. Если признак не влияет на нее, то его коэффициент будет стремиться к нулю. А свободный параметр показывает уровень целевой переменной при нулевых значениях признака.

Интерпретируемость модели машинного обучения - это очень важная характеристика, особенно в некоторых серах применения, например, в медицинской диагностике, государственном управлении, социальной сфере. В таких предметных областях нужно не просто строить качественные модели, но и уметь объяснять их предсказания, понимать, почему и за счет чего было принято такое решение. Поэтому при прочих равных более интерпретируемая модель всегда будет предпочтительнее.

{% capture notice %}
Выводы:
1. Линейные модели могут служить как моделями классификации, так и моделями регрессии.
1. По сути, это одна модель, которую приспосабливают для решения разных задач.
1. Линейная модель - это результат вычисления линейной комбинации признаков и параметров: $ h_b (x) = X \cdot \vec{b} $
1. Для задач регрессии мы сравниваем результат $ X \cdot \vec{b} $ с $ y $.
1. Для задач классификации мы строим разделяющее множество $ X \cdot \vec{b} = 0 $.
1. Эффективность линейных моделей зависит от структуры данных (для регрессии это должна быть линейная корреляция, для классификации - линейная разделимость). 
1. Линейные модели достаточно просты в обработке, но склонны к недообучению из-за своей простоты.
1. Линейные модели интерпретируемы - их коэффициентам можно придать физический смысл.
<!-- 1. Линейные модели эквивалентны одному нейрону нейронной сети. -->
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Полиномиальные модели

![Линейная граница](/assets/images/ml_text/ml2-9.png "Линейная граница"){: .align-center style="width: 50%;"}

Линейные модели обладают неоспоримыми достоинствами, когда мы имеем дело с датасетом определенной структуры. Например, на графике выше представлен очевидно линейно разделимый датасет. И при его анализе более сложные модели даже и не нужны, со всем справится простая логистическая регрессия. Напомним, как выглядит функция модели:

{% capture block %}
$$ h_b (x) = g(b_0 + b_1 x_1 + b_2 x_2) $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

Эта самая простая модель показывает себя хорошо даже и во многих реальных задачах. Особенно если в датасете собрано множестве признаков, есть неплохой шанс на появление линейных связей в данных. Во всяком случае, линейную модель стоит протестировать одной из первых.

Но зачастую мы имеем такие данные, которые просто не могут быть описаны линейной моделью. В таком случае, стоит строить более сложные модели. Мы уже говорили о полиномиальных моделях для задач регрессии. Но они с равным успехом могут применяться и для классификации. Рассмотрим, например, такой набор данных.

![Нелинейная граница](/assets/images/ml_text/ml2-10.png "Нелинейная граница"){: .align-center style="width: 50%;"}

Здесь тоже очень четко прослеживается граница между двумя классами, то есть потенциальная граница принятия решения. Только очевидно, что она носит не линейный характер. Для того, чтобы построить такую границу нам можно взять более сложную функцию, содержащую признаки во второй степени (так как мы подозреваем, что граница принятия решений является кривой второго порядка). Например, возьмем такую функцию:

{% capture block %}
$$ h_b(x) = g(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1^2 + b_4 x_2^2) $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

{% capture notice %}
Конечно, когда в наборе данных больше двух признаков, такие простые графики не получится построить. Поэтому очень сложно заранее сказать, есть ли преобладающая линейная зависимость или данные описываются нелинейными моделями. Здесь мы рисуем эти графики, чтобы показать, как устроены и как работают модели. Они не используются для подбора моделей. Это вообще зачастую выбор "вслепую".
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Мы говорили, что полиномиальную модель можно рассматривать двумя способами - как модель более сложного класса и как добавление новых признаков к набору данных и, таким образом, переход в пространство боле высокой размерности. Рассмотрим простой пример. Выведем на печать матрицу признаков с графика выше:

```python
array([[-0.31, -0.68],
       [ 0.36, -0.17],
       [ 0.17, -0.26],
       [-0.07,  0.26],
       [-0.85, -0.16],
       [-0.13, -0.4 ],
       [ 0.11, -0.15],
       [ 0.19,  0.32],
       [-0.77, -0.55],
       [-0.01,  0.22],
       [ 0.73,  0.66],
       [ 0.33,  0.99],
       [ 0.21,  0.26],
       [-0.39,  1.1 ],
       [ 0.46, -0.89],
       [ 1.07, -0.16],
       [-0.35, -0.18],
       [-0.92,  0.77],
       [-0.27, -0.01],
       [ 0.92, -0.5 ]])
```

Анализируя график можно заметить, что синие точки (положительный класс) сосредоточены ближе к началу координат, а красные - дальше от него. Расстояние от начала координат для каждой точки можно вычислить как сумму квадратов ее координат. Давайте добавим в матрицу третий столбец - еще один признак - сумму квадратов первых двух признаков:

```python
X1 = np.append(X, (X[:, 0] ** 2 + X[:, 1] ** 2).reshape((-1, 1)), axis=1)
```

```python
array([[-0.31, -0.68,  0.56],
       [ 0.36, -0.17,  0.16],
       [ 0.17, -0.26,  0.1 ],
       [-0.07,  0.26,  0.07],
       [-0.85, -0.16,  0.75],
       [-0.13, -0.4 ,  0.18],
       [ 0.11, -0.15,  0.03],
       [ 0.19,  0.32,  0.14],
       [-0.77, -0.55,  0.89],
       [-0.01,  0.22,  0.05],
       [ 0.73,  0.66,  0.97],
       [ 0.33,  0.99,  1.09],
       [ 0.21,  0.26,  0.11],
       [-0.39,  1.1 ,  1.36],
       [ 0.46, -0.89,  1.  ],
       [ 1.07, -0.16,  1.17],
       [-0.35, -0.18,  0.16],
       [-0.92,  0.77,  1.44],
       [-0.27, -0.01,  0.08],
       [ 0.92, -0.5 ,  1.09]])
```

Целевую переменную мы, конечно, не трогаем. Давайте посмотрим на трехмерный график - расположение этих точек в пространстве более высокой размерности:

![Переход в пространство более высокой размерности](/assets/images/ml_text/ml3-51.png "Переход в пространство более высокой размерности"){: .align-center style="width: 70%;"}

Мы видим, что за счет введения третьего признака набор данных стал очень даже линейно разделимым. Теперь синие точки располагаются ниже красных и их можно отделить плоскостью. В данном конкретном случае, плоскость будет горизонтальна, то есть при разделении мы будем использовать вообще только третий признак. В общем случае это не выполняется. Важно, что точки стали располагаться так, что их теперь можно будет разделить, например, вот так:

![Переход в пространство более высокой размерности](/assets/images/ml_text/ml3-52.png "Переход в пространство более высокой размерности"){: .align-center style="width: 70%;"}

То есть после добавления полиномиального признака мы используем обычную линейную модель, но в надпространстве. Откуда же берутся нелинейные границы принятия решений? Это происходит когда мы пытаемся изобразить эту границу в изначальном двумерном пространстве. Ведь изначальная плоскость двумерного графика в этом трехмерном пространстве будет представляться поверхностью второго порядка вот так:

![Переход в пространство более высокой размерности](/assets/images/ml_text/ml3-53.png "Переход в пространство более высокой размерности"){: .align-center style="width: 70%;"}

Пересечение этой поверхности с нашей плоскостью принятия решения как раз и формирует границу, которую модель строит на двумерном графике. Получается кривая второго порядка. Она может выглядеть примерно так:

![Переход в пространство более высокой размерности](/assets/images/ml_text/ml3-54.png "Переход в пространство более высокой размерности"){: .align-center style="width: 70%;"}

{% capture notice %}
На этом примере мы, кстати, можем видеть, почему так труден анализ многомерных данных. Наш изначальный двумерный график с точками по сути своей представляет собой проекцию трехмерного графика на горизонтальную плоскость. Когда мы изображаем два каких-то признака из множества, мы всегда берем проекцию. Так вот, в трехмерном графике абсолютно очевидна линейная разделимость данных. Однако, когда мы анализируем проекцию, данные становятся как бы "вперемешку". Поэтому по проекции нельзя судить о наличии или отсутствии структуры в данных.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

На практике мы не можем знать, какой именно искусственный признак даст нам разделимость выборки в пространстве более высокой размерности. Поэтому обычно добавляют все возможные комбинации всех признаков до определенной степени полинома. В данном случае, для получения полинома второй степени нам нужно было еще добавить признаки $x_1^2$ и $x_2^2$. Они бы не очень влияли на расположение границы принятия решений, но линейные модели могут справится с "лишними" признаками в данных - коэффициенты при них будут просто близкими к нулю. 

Но в общем случае, могут использоваться все признаки в полиноме. Если у нас два изначальных атрибута и мы строим полином второй степени, то у нас получится пять признаков в модели - $x_1, x_2, x_1^2, x_2^2, x_x x_2$. Тогда граница принятия решений может быть любой кривой второго порядка. А может быть и так:

![Полиномиальная классификация](/assets/images/ml_text/ml3-13.png "Полиномиальная классификация"){: .align-center style="width: 50%;"}

Ведь граница принятия решения - это по сути область пересечения гиперплоскости с поверхностью второго порядка. Это пересечение может иметь и более сложную форму.

Конечно, при работе с библиотечными функциями не придется руками перебирать все возможные полиномиальные комбинации. Поэтому достаточно легко можно построить полиномиальную классификацию, например, 10-го порядка:

![Полиномиальная классификация](/assets/images/ml_text/ml3-14.png "Полиномиальная классификация"){: .align-center style="width: 50%;"}

Она уже выглядит гораздо более сложной. За счет того, что она содержит гораздо больше признаков, она более вариабельна, и поэтому способна улавливать более сложные зависимости в данных и строить более сложные границы принятия решений.

Естественно, если мы представляем полиномиальные модели как модификацию исходного набора данных, а именно - добавление к нему новых признаков, то ничего не мешает применять этот пример как для регрессионных, так и для классификационных задач. Ниже вы видите пример полиномиальной модели регрессии со степенью полинома 2:

![Полиномиальная регрессия](/assets/images/ml_text/ml3-15.png "Полиномиальная регрессия"){: .align-center style="width: 50%;"}

А на следующем графике - степень выросла до 7.Обратите внимание, насколько более сложной стала функция модели. Но в данном конкретном примере она все равно далека от того, чтобы пройти точно через все точки.

![Полиномиальная регрессия](/assets/images/ml_text/ml3-16.png "Полиномиальная регрессия"){: .align-center style="width: 50%;"}

{% capture notice %}
Здесь и далее мы уже будем употреблять термины "атрибут" и "признак" более точно. Раньше для нас это были синонимы. Так и есть при рассмотрении линейных моделей. Но, срого говоря, атрибут - это характеристика объекта реального мира. Оно зафиксировано в наборе данных и каждый атрибут обладает определенным смыслом в предметной области. Признак - это вектор, который подается на вход модели машинного обучения. Мы можем использовать как признаки сами атрибуты. Можем создавать новые признаки на основе существующих атрибутов. Можем удалять атрибуты из данных, так что они не становятся признаками. А можем модифицировать атрибут (например, нормировать его) и он превратится в другой признак.
{% endcapture %}
<div class="notice--danger">{{ notice | markdownify }}</div>

Полиномиальные модели как для регрессии, так и для классификации - это универсальные аппроксиматоры. Ведь полиномом достаточно большой степени можно приблизить любую функцию. Этим, кстати и занимаются ряды Тейлора в математическом анализе. Поэтому с помощью полиномиальных моделей мы может решать задачи любой сложности. Однако, при высоких степенях полиномиальные модели могут быть не очень эффективны. Ведь количество признаков экспоненциально растет при увеличении степени. Если у нас изначально было два атрибута, то для второй степени получится, как мы уже считали, пять признаков, для третьей - 11, а для четвертой - уже 21. Но если изначально у нас было больше атрибутов, например, 10, то все еще хуже. Для второй степени понадобится 55 признаков, а для третьей - больше 500. В целом количество признаков растет экспоненциально. Так что если у вас изначально многопризнаков или задача очень сложная, то полиномиальные модели будут работать очень медленно.

{% capture notice %}
Выводы:
1. Добавление полиномиальных признаков возможно как к регрессионным, так и к классификационным моделям.
1. Полиномиальная регрессия позволяет охватывать нелинейные зависимости атрибутов и целевой переменной.
1. Полиномиальная классификация позволяет очерчивать нелинейные границы принятия решений.
1. Здесь и далее: атрибуты - характеристики объектов, данные в датасете; признаки - компоненты вектора, подающегося на вход модели машинного обучения.
1. Полиномиальные модели универсальны, но очень дороги при высоких порядках полинома.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Метод опорных векторов

Вернемся на время к случаю линейно разделимых данных для задач классификации. Как мы знаем, для проведения классификации таких данных достаточно линейной модели. В случае линейно разделимых данных логистическая регрессия ищет какую-нибудь разделяющую гиперплоскость. Главное условие - чтобы точки разных классов оказались по разные стороны этой границы. На графике мы видим пример:

![Оптимальная граница](https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Classifier.svg/1280px-Classifier.svg.png "Оптимальная граница"){: .align-center style="width: 50%;"}
Источник: [Wikipedia](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2).
{: style="text-align: center; font-size:0.7em;"}

Очевидно, что в случае такого расположения точек данных можно провести несколько разделяющих прямых. Каждая их них соответствует модели классификации. И у каждой из них будет ошибка, почти равная нулю, ведь они все разделяют данные идеально. Но при взгляде на такой график любому становится очевидно, что одна из разделяющих линий предпочтительнее других. Как вы думаете, какая?

Большинство людей ответит, что вторая. Однако, сложнее объяснить, почему мы так думаем. С формальной точки зрения, между ними нет никакой разницы, ведь они все дают нулевую ошибку. Но первая и третья линии проходят подозрительно близко к точкам одной из групп. Почему мы интуитивно считаем, что это плохо.

Следует всегда помнить, что задача модели машинного обучение - не предсказание правильного ответа для обучающей выборки. Мы его и так знаем. Но на примере этой выборки модель должна выявить зависимости в данных, которые позволят классифицировать объекты новые, которых модель еще не видела. Это свойство моделей называется обобщающей способностью. Об этом мы еще поговорим дальше. 

Возьмем, например, первую линию. Она прекрасно разделяет те точки, которые мы видим. Но если заставить эту модель классифицировать точку, которая отстоит немного влево от основной группы синих точек, она может дать сбой. Пока мы просто предполагаем такой случай. Позже мы научимся формально его описывать и исключать. 

Поэтому чем ближе граница принятия решений к середине между группами точек, принадлежащих разным классам, чем дальше она от самих этих групп, тем надежнее выглядят предсказания данной модели. Это не имеет непосредственного отношения к измерению эффективности моделей, но придает нам уверенность в результативности и обобщающей способности модели.

Это расстояние между границей принятия решения и крайними точками выборки, принадлежащими разным классам называется зазором между границей и точками. Можно поставить задачу нахождения разделяющей гиперплоскости максимального зазора. То есть из всех границ принятия решений мы будем выбирать лучшую, ориентируясь не только на значение логарифмической функции ошибки, но и на величину зазора.

Опять, как и раньше мы начнем с модификации уже известного нам алгоритма логистической регрессии. А модифицировать его мы будем так, чтобы он максимизировал зазор между границей и точками. Для этого мы модифицируем функцию ошибки таким образом, чтобы

{% capture block %}
$$ y = 1 \rightarrow X \cdot \vec{b} \ge 1 $$

$$ y = 0 \rightarrow X \cdot \vec{b} \le -1 $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

Раньше, в классической модели логистической регрессии, ошибка плавно уменьшалась при стремлении значения линейной комбинации (до передачи в логистическую функцию) в правильную бесконечность. То есть, если реальное значение $y$ у данной точки - 0 (правый график), то есть точка принадлежит отрицательному классу, то чем меньше значение $z$, тем ближе ошибка в данной точке к нулю. При росте $z$ - ошибка неограниченно растет до бесконечности. Если же истинное значение $y = 1$ (левый график), то есть данная точка принадлежит положительному классу, то все наоборот - чем больше $z$, тем ближе ошибка к нулю. Помните, что формулировка функции ошибки - это наша цель оптимизации. То есть, используя именно эту функцию ошибки мы "заставляем" модель принимать правильные значения. И, что важнее, "штрафуем" ошибки. Причем, величина штрафа плавно увеличивается при увеличении степени ошибки.

![Функция ошибки](/assets/images/ml_text/ml2-5.png "Функция ошибки"){: .align-center style="width: 800px;"}

Однако, если мы хотим принять во внимание зазор, то мы должны усилить формулировку функции ошибки. В частности, мы вообще не будем штрафовать значения $z$, которые дают предсказания, близкие к истинному значению целевой переменной. А ошибочные значения будут штрафоваться пропорционально их отклонению от "идеального" значения.

Опять, рассмотрим два случая в каждой точке обучающей выборки. Допустим, истинное значение - 0 (правый график). Тогда при значениях $z <= -1$ мы вообще не штрафуем модель. То есть, говорим, что ошибка в этой точке равна нулю. Не приближается, а именно точно равна. Если же $z > -1$, то ошибка линейно возрастает пропорционально удалению от этой точки. Если же $y = 1$ (левый график) - то мы не штрафуем значения $z >= 1$. а остальные - так же пропорционально удалению.

![Ошибка в опорных векторах](/assets/images/ml_text/ml3-10.png "Ошибка в опорных векторах"){: .align-center style="width: 50%;"}

Обратите внимание, что такая ошибка подразумевает, что некоторый диапазон значений $z$ штрафуется в любом случае. Но в каждом случае, есть некий порог, при преодолении которого ошибка обращается в ноль. И так же, как и в классической модели логистической регрессии, полная ошибка вычисляется как сумма ошибок в каждой точке. 
Такая ошибка позволяет максимизировать зазор между разделяющей гиперплоскостью и крайними точками выборки. Именно за счет такого усиления формы ошибки. А эти крайние точки и называются опорными векторами.

Однако, есть одна проблема практического свойства. Как вы могли заметить, такая функция ошибки не везде дифференцируема. Поэтому для реализации такой модели мы прибегаем к трюку. Можно заметить, что положение нашей границы принятия решений определяется исключительно положением этих крайних точек, то есть опорных векторов.

![Опорные вектора](https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/SVM_margins.svg/1280px-SVM_margins.svg.png "Опорные вектора"){: .align-center style="width: 50%;"}
Источник: [Wikipedia](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2).
{: style="text-align: center; font-size:0.7em;"}

На данном графике показаны опорные вектора (точки), которые определяют "полосу" нейтральной зоны. И граница принятия решения должна располагаться ровно посередине этой полосы. Причем, в зависимости от взаимного положения точек и их принадлежности разным классам, опорных векторов может быть разное количество. Но их будет как минимум два - по одной точке от каждого класса.

Поэтому вместо того, чтобы вычислять функцию ошибки, нам нужно найти в обучающей выборке те самые опорные вектора. В двумерной задаче это может быть сделано достаточно просто - мы можем найти такие точки выборки, которые имеют наименьшее расстояние от точек противоположного класса. Например, рассмотрим такую выборку:

![](/assets/images/ml_text/ml3-55.png "Два опорных вектора"){: .align-center style="width: 50%;"}

Эта выборка соответствует такому расположению точек:

```python
X = np.array([
    [1, 2],
    [2, 1],
    [1, 1],
    [2, 3],
    [3, 2],
    [3, 3],
    [1.8, 1.8],
    [2.2, 2.2]
])
Y = np.array([0, 0, 0, 1, 1, 1, 0, 1])
```

Мы можем найти матрицу расстояний от каждой точки до каждой другой:

```python
X1 = np.array([[distance(p1, p2) for p1 in X] for p2 in X])
```

```python
[[0.  , 2.  , 1.  , 2.  , 4.  , 5.  , 0.68, 1.48],
[2.  , 0.  , 1.  , 4.  , 2.  , 5.  , 0.68, 1.48],
[1.  , 1.  , 0.  , 5.  , 5.  , 8.  , 1.28, 2.88],
[2.  , 4.  , 5.  , 0.  , 2.  , 1.  , 1.48, 0.68],
[4.  , 2.  , 5.  , 2.  , 0.  , 1.  , 1.48, 0.68],
[5.  , 5.  , 8.  , 1.  , 1.  , 0.  , 2.88, 1.28],
[0.68, 0.68, 1.28, 1.48, 1.48, 2.88, 0.  , 0.32],
[1.48, 1.48, 2.88, 0.68, 0.68, 1.28, 0.32, 0.  ]]
```

Конечно, на главной диагонали будут нули. Но нам нужно выбрать из этой матрицы минимально значение между двумя точками, принадлежащими разным классам. Такое значение всегда будет. Поэтому хотя бы два опорных вектора будут всегда присутствовать в любой задаче. В нашем случае это расстояние между последней и предпоследней точками, а именно значение 0,32. Значит эти две точки и будут опорными векторами в данной обучающей выборке. 

Рассмотрим другой пример:

![](/assets/images/ml_text/ml3-56.png "Четыре опорных вектора"){: .align-center style="width: 50%;"}

В этой выборке мы удалили две центральные точки. Где же будут опорные вектора? Так же рассмотрим матрицу взаимных расстояний точек:

```python
[[0, 2, 1, 2, 4, 5],
[2, 0, 1, 4, 2, 5],
[1, 1, 0, 5, 5, 8],
[2, 4, 5, 0, 2, 1],
[4, 2, 5, 2, 0, 1],
[5, 5, 8, 1, 1, 0]]
```

Учитывая, что нам нужны расстояния между точками разных классов, находим, что между точками 0-3 и 4-1 минимальное расстояние - 2. Можете проверить самостоятельно - меньшие расстояния в матрице только между точками одного класса. Это, вообще, типичный случай, ведь расстояние между точками - это некоторая мера их сходства. А схожие точки чаще принадлежат одному классу, чем разным.

Получается, что в этой выборке сразу четыре опорных вектора? Да, и такое бывает. Вообще, опорных векторов может быть любое количество, это зависит от взаимного расположения точек в пространстве.

Важно то, что эти опорные вектора однозначно определяют положение границы принятия решений. Если изменить положение хотя бы одного опорного вектора - то положение границы изменится. А если изменить положение точки, которая не является опорным вектором, то граница останется на том же месте. Это позволяет при классификации использовать положение всего нескольких точек выборки.

{% capture notice %}
Следует заметить, что при изменении положения неопорной точки она сама может стать опорной, если приблизится к границе принятия решений. Поэтому, конечно, при изменении данных модель следует пересчитать заново.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Однако, при размерности точек, большей двух, нахождение опорных векторов уже не так тривиально. И эту задачу решают при помощи численной оптимизации. Ставится эта задача так. Граница принятия решения определяется как область точек, скалярное произведение которых с заданным вектором постоянна. Этот заданный вектор вычисляется как линейная комбинация векторов признаков с параметрами. Или, другими словами, взвешенное среднее векторов признаков, где веса выступают параметрами. Вектор параметров в таком случае выступает как нормальный вектор к разделяющей гиперплоскости. 

Причем этот вектор (его обычно обозначают $w$ и он в наших обычных терминах будет состоять из таких компонент: $\vec{b} = {b_1, b_2, ..., b_n}$) не обязательно единичной длины, как обычно в линейной алгебре. Ширина зазора между гиперплоскостью и точками однозначно определяется как величина, обратная к длине нормального вектора. Поэтому задача сводится к минимизации длины нормального вектора, при сохранении условия, что все точки классифицируются правильно. 

Последнее требование можно смягчить, ведь на практике не все выборки являются линейно разделимыми. Поэтому классификатор может допускать некоторую степень неправильных классификаций. Таким образом оптимизироваться будет некоторая величина, состоящая и из длины нормального вектора и из суммарной ошибки классификации. Эта задача решается методом поиска седловой точки функции Лагранжа, рассмотрение которого выходит за рамки данного пособия.

Важно то, что представленная таким образом задача приводит к нахождению разделяющей гиперплоскости максимального зазора. На графике вы можете видеть результат применения данного метода к уже использованным нами данным:

![Классификация методом SVM без ядра](/assets/images/ml_text/ml3-17.png "Классификация методом SVM без ядра"){: .align-center style="width: 50%;"}

Можно видеть, что результат достаточно близок к результату применения линейной регрессии. Вообще, метод опорных векторов в такой формулировке в чем-то аналогичен линейным методам. Он дает линейную границу принятия решений. Но так же может использоваться и для нахождения линейной регрессии:

![Регрессия методом SVM без ядра](/assets/images/ml_text/ml3-18.png "Регрессия методом SVM без ядра"){: .align-center style="width: 50%;"}

В таком виде метод опорных векторов просуществовал с начала 60-х до начала 90-х годов. В это время исследователи догадались, что такая постановка задачи позволяет совершить один трюк, который сильно расширяет применение и вариативность метода опорных векторов. Этот трюк может быть применен для решения линейно неразделимых задач, то есть для построения нелинейных границ принятия решений. Давайте для примера рассмотрим двумерную выборку, точки которой располагаются на плоскости примерно так:

![Нелинейная граница](/assets/images/ml_text/ml3-1.png "Нелинейная граница"){: .align-center style="width: 50%;"}

Очевидно, что данные могут быть разделены некоторой не очень сложной кривой. Но конкретный вид этой кривой достаточно сложно найти. В полиномиальной модели мы просто добавляем полиномиальные признаки "вслепую", используя все комбинации. Это очень неэффективно. Метод опорных векторов тоже позволяет перейти в пространство более высоких размерностей, но без использования "полного перебора" всех комбинаций до заданной степени полинома, а используя так называемый "ядерные функции".

Давайте зададимся вопросом. Можно ли более эффективно задать признаки для данной выборки точек? Глядя на график становится очевидно, что точки положительного класса (синие) расположены двумя пересекающимися примерно круглыми группами. А точки отрицательного класса (красные) расположены вне этих круглых кластеров. Это наводит на мысль, что в качестве признаков в модели мы можем использовать вообще не само расположение этих точек. Например, что если признаком сделать расстояние от данной точки до выбранных "центров кластеров"? Тогда данная проблема станет линейно разделимой. Хотя граница принятия решения будет сложной формы.

Математически доказано, что такая постановка проблемы эквивалента использованию в методе опорного вектора вместо скалярного произведения другой функции - радиально-базисной. Это и называется "ядерной функцией" или "функцией ядра" метода опорных векторов. То есть мы не изменяем саму задачу, но по-другому определяем саму разделяющую гиперплоскость. В данном случае - разделяющей гиперплоскостью будем считать множество точек $x$, которые удовлетворяют следующему равенству:

$$ \sum_i \alpha_i k(x_i, x) = const $$

где $x_i$ - опорные вектора (в нашем примере - центры кластеров), $\alpha_i$ - их веса, а $k$ - ядерная функция (некоторая мера расстояния или сходства между точками).

Если правильно подобрать опорные вектора, то наша выборка станет абсолютно линейно разделимой. Как и раньше мы можем думать об этом преобразовании, как о добавлении новых признаков в модель (в нашем случае происходит даже замена признаков, потому что изначальные атрибуты не используются), либо как формирование нелинейной границы принятия решений в исходном пространстве.

На практике встает вопрос: а как выбирать эти самые опорные вектора? В простых случаях, как на графике выше, мы можем их проставить вручную, но это очень не универсальный способ. Конечно, нужен метод, который находит их автоматически. И вот здесь как раз используется тот же самый математический аппарат, что и в описанном выше линейном случае. Мы не находим и не задаем опорные вектора сами, мы используем все точки обучающей выборки и оптимизируем их веса таким образом, чтобы он был тем выше, чем "больше" точка является опорной.

![Опорные вектора](/assets/images/ml_text/ml3-3.png "Опорные вектора"){: .align-center style="width: 50%;"}

Когда мы рассматривали простой линейный случай, точки выборки у нас могли быть либо опорными, либо нет. Но на самом деле все немного сложнее. Ведь любая гиперплоскость однозначно определяется всего одним вектором - нормальным. А этот нормальный вектор "составляется" как взвешенная сумма векторов, соответствующих всем точкам обучающей выборки. Другими словами, в качестве опорных векторов для начала берут сами точки обучающей выборки. А затем, в процессе обучения, быстро становится понятно, что не все точки "одинаково полезны". У таких не полезных точек вес будет равен или очень близок к нулю. Если же точка близка к центру кластера, то ее вес будет ненулевым.

{% capture notice %}
На самом деле, конечно, гиперплоскость задается не только нормальным вектором, но еще и некоторым числом. Это число характеризует расстояние плоскости от центра координат. Это число - это свободный коэффициент $b_0$. Если помните, вектор $w$ в методе опорных векторов как раз задавался без него. Этот параметр $b_0$ в совокупности с другими параметрами однозначно задает разделяющую гиперплоскость. Здесь присутствует некоторая информационная избыточность, ведь, например, в двух измерениях, разделяющая прямая задается уравнением $b_0 + b_1 x_1 + b_2 x_2 = 0$. Это уравнение имеет 3 параметра, но всего 2 степени свободы. Ведь его можно свободно умножить на любое ненулевое число.
{% endcapture %}
<div class="notice--warning">{{ notice | markdownify }}</div>

Для полного определения нашей нелинейной модели на опорных векторах, нужно лишь охарактеризовать саму "ядерную функцию". Мы уже сказали, что в качестве функции ядра может применяться скалярное произведение векторов. Если мы зафиксируем некоторую точку (вектор) в пространстве, то постоянное скалярное произведение даст нам множество плоскостей, равноудаленных от данной точки. А для решения нашей второй, нелинейной задачи, нам нужна функция, которая даст окружность вокруг данной точки. Такая ядерная функция называется радиально-базисная и формулируется следующим образом:

$$ k(x, x') = e^{-\gamma (x - x')^2} $$

Вид этой функции очень похож на нормальное распределение. Поэтому данная функция еще называется гауссовой. Относительно конкретной точки эта функция показывает некоторую меру сходства. которая убывает при росте евклидова расстояния от нее. Действительно, если мы посчитаем радиально-базисную функцию от двух совпадающих точек, мы получим 1. Если же точки будут бесконечно далеки друг от друга, то значение функции будет стремиться к нулю. В общем случае, эта функция ведет себя как распределение Гаусса, откуда и название.

Применение этой функции в методе опорных векторов означает, что мы строим разделяющую гиперплоскость, выглядящую как множество равноудаленных точек от некоторого средневзвешенного вектора в пространстве высокой размерности. Если мы спроецируем эту границу принятия решений в наше изначальное пространство, то оно будет выглядеть как некоторая кривая (в большинстве случаев замкнутая), похожая на эллипс, то есть кривую второго порядка, но более сложной формы. Сложность границы принятия решений в методе опорных векторов зависит не от нелинейности ядерной функции, а от того, сколько точек выборки метод примет за опорные.

![Классификация методом SVM с гауссовым ядром](/assets/images/ml_text/ml3-19.png "Классификация методом SVM с гауссовым ядром"){: .align-center style="width: 50%;"}

Метод опорных векторов с гауссовым ядром очень редко применяется для решения задач регрессии, так как обладает очень характерным поведением, которое вы можете видет на графике. Это поведение достаточно редко встречается в реальной жизни в данных, предполагающих регрессионные задачи.

![Классификация методом SVM с гауссовым ядром](/assets/images/ml_text/ml3-20.png "Классификация методом SVM с гауссовым ядром"){: .align-center style="width: 50%;"}

Так как метод опорных векторов не привязан к конкретной задаче - регрессии или классификации - в общем виде он обозначается SVM - support vectors machine. Если же мы говорим о его адаптации к задаче регрессии, то можно встретить аббревиатуру SVR - support vectors regressor, то есть регрессор на основе метода опорных векторов. И, соответственно, существует SVC - support vectors classifier, классификатор на опорных векторах.

Следует заметить, что описанный выше алгоритм линейной классификации при помощи метода опорных векторов, когда в качестве ядреной функции применяется скалярное произведение, называется метод опорных векторов с линейным ядром. Он же, по ряду причин, называется метод опорных векторов без ядра. Имейте это в виду, данная терминология может немного ввести в заблуждение. 

В качестве функции расстояния можно брать разные метрики (они должны удовлетворять определенным условиям). С большим отрывом чаще всего применяются на практике линейное и гауссово ядро. А так как линейное ядро почти всегда аналогично применению обычной линейной регрессии, по умолчанию почти всегда применяется именно гауссово ядро. В библиотечных реализациях метода опорных векторов обычно есть возможность выбрать их несольких уже готовых ядерных функций.

{% capture block %}
Наиболее распространенные ядерные функции:
1. Линейное (SVM без ядра): $ (x \cdot x') $
1. Радиальная базисная функция: $ e^{-\gamma \| x - x'\|^2} $
1. Полиномиальное: $ (x \cdot x')^d $
1. Полиномиальное смещенное: $ (x \cdot x' + 1)^d $
1. Сигмоида: $ tanh(k x \cdot x' + c) $
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

Метод опорных векторов с разным типом ядер дает разный тип границы принятия решений. Например, ниже вы видите результат работы классификатора на опорных векторах с полиномиальным ядром. Полиномиальное ядро подразумевает, что предварительно надо задать еще и степень полинома. По умолчанию, и на графиках ниже используется третья степень.

![Классификация методом SVM с полиномиальным ядром](/assets/images/ml_text/ml3-21.png "Классификация методом SVM с полиномиальным ядром"){: .align-center style="width: 50%;"}

Такой же метод опорных векторов с полиномиальным ядром третьей степени, но уже для решения задачи регрессии. Можно заметить, что кривая несомненно сложнее, чем кривая третьего порядка. Ведь сложность границы принятия решения зависит от количества опорных векторов.

![Классификация методом SVM с полиномиальным ядром](/assets/images/ml_text/ml3-22.png "Классификация методом SVM с полиномиальным ядром"){: .align-center style="width: 50%;"}

Метод опорных векторов имеет ряд преимуществ и недостатков по сравнению с линейными и полиномиальными моделями. Он, несомненно, более эффективен в построении нелинейных границ принятия решений, чем полиномиальные модели. Он более универсален, чем линейные. Но надо помнить, что в процессе нахождения опорных векторов в какой-то момент приходится оценивать расстояние между точками выборки. Поэтому данный метод может выполняться довольно медленно, если точек большое количество. Но с другой стороны, если в задаче количество точек сильно меньше, чем количество атрибутов, метод опорных векторов наоборот, может дать преимущество в скорости.

Но метод опорных векторов менее интерпретируем и понятен, чем линейные модели. Результат работы классификатора нельзя интерпретировать как вероятность принадлежности объект данному классу, как в линейных и полиномиальных моделях. А это затрудняет его использование в мультиклассовых задачах (то ест в таких, где объект может одновременно принадлежать нескольким классам) и в задачах нечеткой классификации.

{% capture notice %}
Выводы:
1. Метод опорных векторов находит оптимальную разделяющую гиперплоскость, а значит приводит к более уверенной классификации.
1. SVM может реализовывать нелинейные модели за счет применения функций ядер.
1. Метод опорных векторов может использоваться как для регрессии (SVR), так и для классификации (SVC).
1. SVM эффективны, когда количество атрибутов больше количества объектов.
1. SVM очень чувствителен к шуму, выбросам в данных.
1. SVM не дают прямых оценок вероятности принадлежности. 
<!-- 1. Метод опорных векторов эквивалентен однослойной нейронной сети с количеством нейронов на скрытом слое равном количеству опорных векторов. -->
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Перцептрон

Перцептрон - это самый просто вид искусственных нейронных сетей. Вообще, подробное рассмотрение нейронных сетей выходит за рамки этого пособия, это отдельная и очень глубокая тема, требующая подробного изучения. Но именно перцептрон довольно часто используется наряду с другими классическими методами обучения с учителем. Поэтому здесь мы рассмотрим основные принципы работы перцептрона именно для решения относительно простых задач регрессии и классификации, без подробного рассмотрения техник глубокого обучения. Для того, чтобы познакомиться с работой нейронной сети, рассмотрим сначала принцип действия одного искусственного нейрона.

На минутку возвратимся к 

![Нейрон](/assets/images/ml_text/ml3-5.png "Нейрон"){: .align-center style="width: 50%;"}

![Нейрон подробно](/assets/images/ml_text/ml3-42.png "Нейрон подробно"){: .align-center style="width: 50%;"}

![Ступенчатая активация](/assets/images/ml_text/ml3-44.png "Ступенчатая активация"){: .align-center style="width: 50%;"}

![Функции активации](/assets/images/ml_text/ml3-45.png "Функции активации"){: .align-center style="width: 50%;"}

![ReLU](/assets/images/ml_text/ml3-46.png "ReLU"){: .align-center style="width: 50%;"}

![Нейрон подробно](/assets/images/ml_text/ml3-42.png "Нейрон подробно"){: .align-center style="width: 50%;"}

![Нейрон как признаки](/assets/images/ml_text/ml3-7.png "Нейрон как признаки"){: .align-center style="width: 50%;"}

![Перцептрон](/assets/images/ml_text/ml3-6.png "Перцептрон"){: .align-center style="width: 50%;"}

```python
class Perceptron:
    def __init__(self, learning_rate=0.01, n_iters=1000):
        self.lr = learning_rate
        self.n_iters = n_iters
        self.activation_func = self._unit_step_func
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        y_ = np.array([1 if i > 0 else 0 for i in y])
        for _ in range(self.n_iters):            
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_predicted = self.activation_func(linear_output)
                update = self.lr * (y_[idx] - y_predicted)
                self.weights += update * x_i
                self.bias += update

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        y_predicted = self.activation_func(linear_output)
        return y_predicted

    def _unit_step_func(self, x):
        return np.where(x>=0, 1, 0)
```

![Нейросеть по умолчанию](/assets/images/ml_text/ml3-23.png "Нейросеть по умолчанию"){: .align-center style="width: 50%;"}

![Нейросеть на другом наборе данных](/assets/images/ml_text/ml3-24.png "Нейросеть на другом наборе данных"){: .align-center style="width: 50%;"}

![Многослойный перцептрон](/assets/images/ml_text/ml3-8.png "Многослойный перцептрон"){: .align-center style="width: 50%;"}

![Глубокая сеть](/assets/images/ml_text/ml3-9.png "Глубокая сеть"){: .align-center style="width: 50%;"}

![Нейросеть глубокая](/assets/images/ml_text/ml3-25.png "Нейросеть глубокая"){: .align-center style="width: 50%;"}

![Нейросеть на третьем наборе](/assets/images/ml_text/ml3-26.png "Нейросеть на третьем наборе"){: .align-center style="width: 50%;"}

![Нейросеть глубокая для регрессии](/assets/images/ml_text/ml3-27.png "Нейросеть глубокая для регрессии"){: .align-center style="width: 50%;"}

{% capture notice %}
Выводы:
1. Нейронные сети являются самым популярным методом обучения с учителем.
1. Нейронные сети позволяют строить самые сложные и масштабируемые модели.
1. Обучение и применение нейронных сетей очень хорошо распараллеливается. За счет этого можно ускорять их работу с помощью графических ядер и других средств.
1. Перцептрон - самая простая архитектура нейронной сети, состоящая из нескольких полносвязных слоев.
1. Нейронные сети работают по принципу черного ящика, их очень сложно интерпретировать.
1. При применении нейронных сетей встает вопрос выбора архитектуры, количества нейронов.
1. Главный параметр нейронной сети - количество скрытых слоев. Если их больше одного сеть называется глубокой.
1. Нейронные сети очень естественно решают задачу множественной и мультиклассовой классификации.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Деревья решений

Деревья решений - это один из самых простых, но при этом популярных методов обучения с учителем. Они применяются как для построения моделей классификации или регрессии, так и для анализа данных, чтобы получить полезную информацию о некоторых зависимостях в них.

Дерево решений - это иерархическая структура, состоящая из правил вида "Если ..., то...". Примерно так же будет рассуждать человек, которому поставлена задача определить по имеющимся характеристикам объекта значение целевой переменной. В узлах дерева содержатся некоторые условия, или критерии, по отношению к которым объект может быть отнесен к одному из вариантов. Как правило, данные критерии бинарные и выражаются в оценке значения одного из признаков. Чаще всего при использовании непрерывных шкал признаков используется сравнение с пороговым значением.

По результатам проверки в узле мы может пойти по одному из ребер и попасть либо в следующий узел с другой проверкой, либо в листьевой элемент, в котором содержатся значение целевой переменной. Если это задача классификации, то это конкретная метка класса. Если это задача регрессии - то это чаще всего конкретное значение целевой переменной. Однако, может использоваться линейная функция от признаков (В каждом листе - разная). Таким образом решается задача моделирования целевой переменной.

![Схема дерева решений](https://loginom.ru/sites/default/files/blogpost-files/activation-function.svg "Схема дерева решений"){: .align-center style="width: 80%;"}
Источник: [Loginom](https://loginom.ru/blog/decision-tree-p1).
{: style="text-align: center; font-size:0.7em;"}

В машинном обучении деревья стоит задача нахождения оптимального дерева решений - такого дерева, который наилучшим образом предсказывает значение целевой переменной. Обучение - это подбор параметров. Параметрами дерева являются конкретные значения порогов в каждом узле. Также скрытым параметром является порядок "опроса" признаков.

Построение дерева решений - это итеративный процесс, в котором на каждой итерации находится оптимальный признак и его пороговое значение, которое лучше всего разбивает выборку на две подвыборки. Затем процесс повторяется для каждой из двух подвыборок, пока не будет достигнуто некоторое условие остановки алгоритма. Процесс разбиения выборки может окончится двумя способами. Естественный - это когда в выборке остаются объекты только одного из классов. Искусственный - это достижение критерия остановки. В качестве критерия чаще всего используется максимальная глубина дерева. Но можно задать, например, минимально количество объектов в узле.

Алгоритм построения дерева - это жадный алгоритм типа "разделяй и властвуй". Существует несколько алгоритмов построения деревьев:

1. ID3
1. C4.5
1. C5.0
1. CART

Самое главное в алгоритме построения дерева - выбор критерия разбиения. Чаще всего используется теоретико-информационный критерий, основанный на оценке количества информации до и после разбиения. Информация выборки определяется по следующей формуле:

$$ - \sum_{i=1}^{n} \frac{N_i}{N} log(\frac{N_i}{N})$$

Статистическийкритерий основан на использовании коэффициента Джинни.

![Информационный критерий разбиения](/assets/images/ml_text/ml3-46.png "Информационный критерий разбиения"){: .align-center style="width: 50%;"} 

```python
clf = DecisionTreeClassifier(max_depth=3)
plot_model(clf)
```

![Дерево решений глубиной 1](/assets/images/ml_text/ml3-28.png "Дерево решений глубиной 1"){: .align-center style="width: 50%;"}

![Дерево решений глубиной 2](/assets/images/ml_text/ml3-29.png "Дерево решений глубиной 2"){: .align-center style="width: 50%;"}

![Дерево решений глубиной 3](/assets/images/ml_text/ml3-30.png "Дерево решений глубиной 3"){: .align-center style="width: 50%;"}

![Дерево решений глубиной 4](/assets/images/ml_text/ml3-31.png "Дерево решений глубиной 4"){: .align-center style="width: 50%;"}

![Дерево решений глубиной 5](/assets/images/ml_text/ml3-32.png "Дерево решений глубиной 5"){: .align-center style="width: 50%;"}

![Дерево решений глубиной 21](/assets/images/ml_text/ml3-33.png "Дерево решений глубиной 21"){: .align-center style="width: 50%;"}

```python
from sklearn.tree import plot_tree
plt.figure(figsize=(12, 9))
plot_tree(clf)
```

![Вывод дерева](/assets/images/ml_text/ml3-47.png "Вывод дерева"){: .align-center style="width: 50%;"}

![Дерево решений для регрессии](/assets/images/ml_text/ml3-34.png "Дерево решений для регрессии"){: .align-center style="width: 50%;"}

![Вывод дерева](/assets/images/ml_text/ml3-48.png "Вывод дерева"){: .align-center style="width: 50%;"}

{% capture notice %}
Выводы:
1. Дерево решений - это граф, в узлах которого содержатся некоторые условия (как правило, пороговые условия по признаку)
1. Деревья решений являются одним из самых популярных методов классического машинного обучения. Используются как для построения модели, так и для предварительного анализа данных.
1. Дерево решений просто построить, ему не требуются огромные вычислительные мощности.
Деревья решений - интерпретируемый метод, его можно объяснить и понять, особенно при малых глубинах дерева.
1. Деревья решений более лояльны к входным данным - могут работать с категориальными признаками.
1. Деревья решений склонны к переобучению при достаточно больших глубинах.
1. Дерево решений очень толерантно к лишним признакам в наборе данных.
1. Существуют специальные виды распределений, которые сложно описываются деревьями решений.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### K ближайших соседей

![Классификация по 1 ближайшему соседу](/assets/images/ml_text/ml3-35.png "Классификация по 1 ближайшему соседу"){: .align-center style="width: 50%;"}

![Классификация по 5 ближайшему соседу](/assets/images/ml_text/ml3-36.png "Классификация по 5 ближайшему соседу"){: .align-center style="width: 50%;"}

![Классификация по 20 ближайшему соседу](/assets/images/ml_text/ml3-37.png "Классификация по 20 ближайшему соседу"){: .align-center style="width: 50%;"}

![Регрессия методом ближайших соседей](/assets/images/ml_text/ml3-38.png "Регрессия методом ближайших соседей"){: .align-center style="width: 50%;"}

![Ближайшие соседи](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/1920px-KnnClassification.svg.png "Ближайшие соседи"){: .align-center style="width: 50%;"}
Источник: [Википедия](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9).
{: style="text-align: center; font-size:0.7em;"}

```py
import numpy as np
from collections import Counter

def euclidean_distance(x1, x2): return np.sqrt(np.sum((x1 - x2)**2))

class KNN:
    def __init__(self, k=3):
        self.k = k
    def fit(self, X, y):
        self.X_train = X
        self.y_train = y
    def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)
    def _predict(self, x):
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]
        k_idx = np.argsort(distances)[:self.k]
        k_neighbor_labels = [self.y_train[i] for i in k_idx]
        most_common = Counter(k_neighbor_labels).most_common(1)
        return most_common[0][0]
```

Минимакс нормализация:

$$ x' = \frac{x - x_{min}}{x_{max} - x_{min}} $$

Z-нормализация:

$$ x' = \frac{x - M[x]}{\sigma_x} $$

{% capture notice %}
Выводы:
1. Метод ближайших соседей заключается в предсказании значения исходя из значения ближайших к нему объектов обучающей выборки.
1. Чаще всего используется евклидова метрика расстояния между объектами.
1. Не имеет внутренних параметров. Можно считать, что параметры - это сама обучающая выборка.
1. Нужно выбрать k - количество используемых соседей.
1. Чем больше k, тем проще модель и больше вероятность недообучения.
1. Самый простой алгоритм для классификации.
1. Опирается на гипотезу компактности: схожие объекты чаще принадлежат одному классу, чем разным.
1. Данные обязательно надо нормализовать.
1. Алгоритм не чувствителен к выбросам.
1. Алгоритм работает медленно при большом объеме обучающей выборки.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Наивная байесовская модель

{% capture block %}
$$ P(A \| B) = \frac{P(B \| A) \cdot P(A)}{P(B)} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

$ P(A \| B) $ - апостериорная вероятность
$ P(A) $ - априорная вероятность
$ P(B \| A) $ - вероятность признака, при данном классе
$ P(B) $ - полная вероятность признака

{% capture block %}
$$ P(y \| x_1, x_2, ..., x_n) = \frac{P(y) \cdot \prod_{i=1}^{n} P(x_i \| y)}{P(x_1, x_2, ..., x_n)} $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>


```python
class NaiveBayes:
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self._classes = np.unique(y)
        n_classes = len(self._classes)

        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)
        self._var = np.zeros((n_classes, n_features), dtype=np.float64)
        self._priors =  np.zeros(n_classes, dtype=np.float64)

        for idx, c in enumerate(self._classes):
            X_c = X[y==c]
            self._mean[idx, :] = X_c.mean(axis=0)
            self._var[idx, :] = X_c.var(axis=0)
            self._priors[idx] = X_c.shape[0] / float(n_samples)

    def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)

    def _predict(self, x):
        posteriors = []
        for idx, c in enumerate(self._classes):
            prior = np.log(self._priors[idx])
            posterior = np.sum(np.log(self._pdf(idx, x)))
            posterior = prior + posterior
            posteriors.append(posterior)
        return self._classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self._mean[class_idx]
        var = self._var[class_idx]
        numerator = np.exp(- (x-mean)**2 / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator
```

```python
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
```

![Классификация методом наивного Байеса](/assets/images/ml_text/ml3-39.png "Классификация методом наивного Байеса"){: .align-center style="width: 50%;"}

{% capture notice %}
Выводы:
1. Наивный байесовский классификатор - модель, основанная на применении формулы Байеса.
1. Этот метод полагается на предположение о независимости признаков.
1. Хорошо обучается на малых наборах данных.
1. Наивные байесовские классификаторы очень быстро работают.
1. Все параметры классификатора могут быть аппроксимированы относительными частотами из обучающей выборки.
1. Если какое-то значение не встречается в обучающей выборке, то его вероятность будет равна нулю и даст некорректную классификацию.
1. Алгоритм чаще всего применяется при анализе текстов - рубрикация, определение тональности, фильтрация спама. 
1. Эффективность можно сильно повысить при помощи инжиниринга признаков.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

<!-- ### Гауссовский процесс

![Классификация методом Гауссовых процессов](/assets/images/ml_text/ml3-40.png "Классификация методом Гауссовых процессов"){: .align-center style="width: 50%;"}

![Регрессия методом Гауссовых процессов](/assets/images/ml_text/ml3-41.png "Регрессия методом Гауссовых процессов"){: .align-center style="width: 50%;"}

{% capture notice %}
Выводы:
1. 
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div> -->