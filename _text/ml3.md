---
section: ml
title: "Методы обучения с учителем"
---

### Линейные модели

![Регрессия и классификация](https://www.researchgate.net/profile/Christian-Bauckhage/publication/330760775/figure/fig3/AS:721129559822338@1548942093486/binary-linear-classification-in-3D.png "Регрессия и классификация"){: .align-center style="width: 50%;"}
Источник: [ResearchGate](https://www.researchgate.net/publication/330760775_Lecture_Notes_on_Machine_Learning_Binary_Linear_Classifiers/figures?lo=1&utm_source=google&utm_medium=organic).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Линейные модели могут служить как моделями классификации, так и моделями регрессии.
1. По сути, это одна модель, которую приспосабливают для решения разных задач.
1. Линейная модель - это результат вычисления линейной комбинации признаков и параметров: $ h_b (x) = X \cdot \vec{b} $
1. Для задач регрессии мы сравниваем результат $ X \cdot \vec{b} $ с $ y $.
1. Для задач классификации мы строим разделяющее множество $ X \cdot \vec{b} = 0 $.
1. Эффективность линейных моделей зависит от структуры данных (для регрессии это должна быть линейная корреляция, для классификации - линейная разделимость).
1. Линейные модели эквивалентны одному нейрону нейронной сети.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Полиномиальные модели

![Линейная граница](/assets/images/ml_text/ml2-9.png "Линейная граница"){: .align-center style="width: 50%;"}

{% capture block %}
$$ h_b (x) = g(b_0 + b_1 x_1 + b_2 x_2) $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

![Нелинейная граница](/assets/images/ml_text/ml2-10.png "Нелинейная граница"){: .align-center style="width: 50%;"}

{% capture block %}
$$ h_b(x) = g(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_1^2 + b_4 x_2^2) $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

![Полиномиальная классификация](https://i.stack.imgur.com/OV3Fp.png "Полиномиальная классификация"){: .align-center style="width: 800px;"}
Источник: [Data Science Stack exchange](https://datascience.stackexchange.com/questions/85441/non-linear-decision-boundary-in-logistic-regression-algorithm-with-polynomial-fe).
{: style="text-align: center; font-size:0.7em;"}

{% capture notice %}
Выводы:
1. Добавление полиномиальных признаков возможно как к регрессионным, так и к классификационным моделям.
1. Полиномиальная регрессия позволяет охватывать нелинейные зависимости атрибутов и целевой переменной.
1. Полиномиальная классификация позволяет очерчивать нелинейные границы принятия решений.
1. Здесь и далее: атрибуты - характеристики объектов, данные в датасете; признаки - компоненты вектора, подающегося на вход модели машинного обучения.
1. Полиномиальные модели универсальны, но очень дороги при высоких порядках полинома.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### Метод опорных векторов

В случае линейно разделимых данных логистическая регрессия ищет какую-нибудь разделяющую гиперплоскость. Можно поставить задачу нахождения разделяющей гиперплоскости максимального зазора.

![Оптимальная граница](https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Classifier.svg/1280px-Classifier.svg.png "Оптимальная граница"){: .align-center style="width: 50%;"}
Источник: [Wikipedia](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2).
{: style="text-align: center; font-size:0.7em;"}

Для этого мы модифицируем функцию ошибки таким образом, чтобы

{% capture block %}
$$ y = 1 \rightarrow X \cdot \vec{b} \ge 1 $$

$$ y = 0 \rightarrow X \cdot \vec{b} \le -1 $$
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

Такая ошибка позволяет максимизировать зазор между разделяющей гиперплоскостью и крайними точками выборки:

![Ошибка в опорных векторах](/assets/images/ml_text/ml3-10.png "Ошибка в опорных векторах"){: .align-center style="width: 50%;"}

Эти крайние точки и называются опорными векторами.

Мы опустим полную математическую формализацию метода опорных векторов и оптимизации параметров метода опорных векторов.

Скажем лишь, что разделяющая гиперплоскость определяется теперь именно как функция от опорных векторов.

![Опорные вектора](https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/SVM_margins.svg/1280px-SVM_margins.svg.png "Опорные вектора"){: .align-center style="width: 50%;"}
Источник: [Wikipedia](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2).
{: style="text-align: center; font-size:0.7em;"}

В таком виде метод опорных векторов просуществовал с начала 60-х до начала 90-х годов. В это время исследователи догадались, что такая постановка задачи позволяет совершить один трюк, который сильно расширяет применение и вариативность метода опорных векторов.

![Нелинейная граница](/assets/images/ml_text/ml3-1.png "Нелинейная граница"){: .align-center style="width: 50%;"}

Можно ли более эффективно задать признаки для модели линейной классификации? Например, если признаком сделать расстояние от данной точки до выбранных "центров кластеров"? Тогда данная проблема станет линейно разделимой. Хотя граница принятия решения будет сложной формы.

На практике в качестве опорных векторов для начала берут сами точки обучающей выборки. А хатем, в процессе обучения, быстро становится понятно, что не все точки "одинаково полезны".

![Опорные вектора](/assets/images/ml_text/ml3-3.png "Опорные вектора"){: .align-center style="width: 50%;"}

В качестве функции расстояния можно брать разные метрики (они должны удовлетворять сложным условиям).

{% capture block %}
Наиболее распространенные ядерные функции:
1. Линейное (SVM без ядра): $ (x \cdot x') $
1. Полиномиальное: $ (x \cdot x' + 1)^d $
1. Радиальная базисная функция: $ e^{-\gamma \| x - x'\|^2} $
1. Сигмоида: $ tanh(k x \cdot x' + c) $
{% endcapture %}
<div class="presentation">{{ block | markdownify }}</div>

{% capture notice %}
Выводы:
1. Метод опорных векторов находит оптимальную разделяющую гиперплоскость, а значит приводит к более уверенной классификации.
1. SVM может реализовывать нелинейные модели за счет применения функций ядер.
1. Метод опорных векторов может использоваться как для регрессии (SVR), так и для классификации (SVC).
1. SVM эффективны, когда количество атрибутов больше количества объектов.
1. SVM очень чувствителен к шуму, выбросам в данных.
1. SVM не дают прямых оценок вероятности принадлежности. 
1. Метод опорных векторов эквивалентен однослойной нейронной сети с количеством нейронов на скрытом слое равном количеству опорных векторов.
{% endcapture %}
<div class="notice--info">{{ notice | markdownify }}</div>

### K ближайших соседей

### Перцептрон

![Нейрон](/assets/images/ml_text/ml3-5.png "Нейрон"){: .align-center style="width: 50%;"}

![Нейрон](/assets/images/ml_text/ml3-6.png "Нейрон"){: .align-center style="width: 50%;"}

![Нейрон](/assets/images/ml_text/ml3-7.png "Нейрон"){: .align-center style="width: 50%;"}

![Нейрон](/assets/images/ml_text/ml3-8.png "Нейрон"){: .align-center style="width: 50%;"}

![Нейрон](/assets/images/ml_text/ml3-9.png "Нейрон"){: .align-center style="width: 50%;"}

### Деревья решений

### Наивная байесовская модель

### Гауссовский процесс

### Смешанный гауссов узел